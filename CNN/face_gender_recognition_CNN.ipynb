{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers, optimizers, layers, models, constraints \n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed()\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFW_VS_LFW_DEEPFUNNELED = 'lfw-deepfunneled'\n",
    "INPUT_DATA = 'output_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN PARAMETERS\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.02\n",
    "NUMBER_OF_NEURONS = 1024\n",
    "REGULARIZATION_LAMBDA = 0.05\n",
    "FILTERS_NUMBER_1 = 24\n",
    "FILTERS_NUMBER_2 = 48\n",
    "FILTERS_SIZE = (4,4)\n",
    "\n",
    "# CONVOLUTION PARAMETERS\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 3\n",
    "NUM_OF_FEMALES = 3000\n",
    "NUM_OF_MALES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jj(*args):\n",
    "    return os.path.join(*args)\n",
    "\n",
    "def open_image(full_image_name):\n",
    "#     return Image.open(full_image_name)\n",
    "    flag = 1 if CHANNELS == 3 else 0\n",
    "    return cv2.imread(full_image_name, flag)\n",
    "\n",
    "def resize(img):\n",
    "    return cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "def resize_dataset(dataset):\n",
    "    result = []\n",
    "    for img in dataset:\n",
    "        new_img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        result.append(new_img)\n",
    "    return np.array(result)\n",
    "    \n",
    "def convert_to_array(img):\n",
    "    return np.array(img, ndmin=3)\n",
    "\n",
    "def get_gender_dataset(gender, number_of_imgs):\n",
    "    faces_list = []\n",
    "    for img_name in tqdm(os.listdir(jj(INPUT_DATA, LFW_VS_LFW_DEEPFUNNELED))):\n",
    "        if img_name.startswith(gender):\n",
    "            path = jj(INPUT_DATA, LFW_VS_LFW_DEEPFUNNELED, img_name)\n",
    "            img = open_image(path)\n",
    "            resized_img = resize(img)\n",
    "            array = convert_to_array(resized_img)\n",
    "            label = 0 if gender == 'f' else 1\n",
    "            faces_list.append((array, label))\n",
    "\n",
    "    random.shuffle(faces_list)\n",
    "\n",
    "    return faces_list[:number_of_imgs]\n",
    "\n",
    "def get_dataset():\n",
    "    females = get_gender_dataset('f', NUM_OF_FEMALES)\n",
    "    males = get_gender_dataset('m', NUM_OF_MALES)\n",
    "    dataset = females + males\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset: [(np.array(), np.array()), (...), ...]\n",
    "# CREATED ONCE, TEST DATASET MUST BE UNTOUCH!\n",
    "\n",
    "# dataset = get_dataset()\n",
    "# len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset on train_and_validation dataset and test dataset\n",
    "# CREATED ONCE, TEST DATASET MUST BE UNTOUCH!\n",
    "\n",
    "# train_and_validation, test = train_test_split(dataset, test_size = 0.2, random_state = 0)\n",
    "# x_test = np.asarray([i[0] for i in test])/255\n",
    "# y_test = np.asarray([i[1] for i in test])\n",
    "\n",
    "# WRITE DATASETS\n",
    "# pickle.dump(train_and_validation, open(\"train_and_validation.data\", \"wb\"))\n",
    "# pickle.dump(test, open(\"test.data\", \"wb\"))\n",
    "\n",
    "# # READA DATASETS\n",
    "# train_and_validation = pickle.load(open(\"train_and_validation.data\", \"rb\"))\n",
    "# test = pickle.load(open(\"test.data\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation = train_test_split(train_and_validation, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# x_train = np.asarray([i[0] for i in train])/255\n",
    "# y_train = np.asarray([i[1] for i in train])\n",
    "\n",
    "# x_validation = np.asarray([i[0] for i in validation])/255\n",
    "# y_validation = np.asarray([i[1] for i in validation])\n",
    "\n",
    "# x_test = np.asarray([i[0] for i in test])/255\n",
    "# y_test = np.asarray([i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images, use ONLY to resize DOWN\n",
    "\n",
    "# x_new_train = resize_dataset(x_train)\n",
    "# x_new_validation = resize_dataset(x_validation)\n",
    "\n",
    "# x_train_and_validation = np.asarray([i[0] for i in train_and_validation])/255\n",
    "# y_train_and_validation = np.asarray([i[1] for i in train_and_validation])\n",
    "\n",
    "# x_test_new = resize_dataset(x_test)\n",
    "# x_train_and_validation_new = resize_dataset(x_train_and_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pot loss\n",
    "def show_loss(history):   \n",
    "    x_axis = range(0, EPOCHS)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, history.history['loss'], label='train_loss')\n",
    "    ax.plot(x_axis, history.history['val_loss'], label='val_loss')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Log loss')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.title('Learning curves')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN model\n",
    "\n",
    "def nn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=FILTERS_NUMBER_1,\n",
    "                            kernel_size=FILTERS_SIZE,\n",
    "                            strides=(1,1),\n",
    "                            padding=\"same\",\n",
    "                            activation='relu',\n",
    "                            use_bias=True,                            \n",
    "                            input_shape=(IMG_SIZE, IMG_SIZE, CHANNELS),\n",
    "                           ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(filters=FILTERS_NUMBER_2,\n",
    "                            kernel_size=FILTERS_SIZE,\n",
    "                            strides=(1,1),\n",
    "                            padding=\"same\",\n",
    "                            activation='relu',\n",
    "                            use_bias=True,                          \n",
    "                           ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(NUMBER_OF_NEURONS,\n",
    "                           activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(REGULARIZATION_LAMBDA),\n",
    "                          ))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimazier = optimizers.SGD(lr=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimazier,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD SAVED MODEL TO CONTINUE TRAINING\n",
    "# model = models.load_model(\"cnn_85.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4772 samples, validate on 1193 samples\n",
      "Epoch 1/40\n",
      "4772/4772 [==============================] - ETA: 95s - loss: 95.1928 - acc: 0.520 - ETA: 60s - loss: 94.9057 - acc: 0.545 - ETA: 48s - loss: 94.6270 - acc: 0.533 - ETA: 42s - loss: 94.3461 - acc: 0.530 - ETA: 39s - loss: 94.0649 - acc: 0.538 - ETA: 36s - loss: 93.7867 - acc: 0.536 - ETA: 34s - loss: 93.5085 - acc: 0.538 - ETA: 32s - loss: 93.2321 - acc: 0.541 - ETA: 30s - loss: 92.9570 - acc: 0.537 - ETA: 29s - loss: 92.6806 - acc: 0.547 - ETA: 28s - loss: 92.4063 - acc: 0.547 - ETA: 26s - loss: 92.1332 - acc: 0.547 - ETA: 25s - loss: 91.8618 - acc: 0.546 - ETA: 25s - loss: 91.5905 - acc: 0.553 - ETA: 24s - loss: 91.3222 - acc: 0.552 - ETA: 23s - loss: 91.0525 - acc: 0.563 - ETA: 22s - loss: 90.7852 - acc: 0.567 - ETA: 21s - loss: 90.5185 - acc: 0.573 - ETA: 21s - loss: 90.2516 - acc: 0.581 - ETA: 20s - loss: 89.9899 - acc: 0.575 - ETA: 19s - loss: 89.7274 - acc: 0.573 - ETA: 18s - loss: 89.4651 - acc: 0.575 - ETA: 18s - loss: 89.2038 - acc: 0.578 - ETA: 17s - loss: 88.9438 - acc: 0.579 - ETA: 16s - loss: 88.6856 - acc: 0.578 - ETA: 15s - loss: 88.4270 - acc: 0.580 - ETA: 15s - loss: 88.1697 - acc: 0.585 - ETA: 14s - loss: 87.9144 - acc: 0.586 - ETA: 13s - loss: 87.6595 - acc: 0.587 - ETA: 12s - loss: 87.4056 - acc: 0.589 - ETA: 11s - loss: 87.1525 - acc: 0.591 - ETA: 11s - loss: 86.9003 - acc: 0.592 - ETA: 10s - loss: 86.6486 - acc: 0.593 - ETA: 9s - loss: 86.3988 - acc: 0.594 - ETA: 8s - loss: 86.1493 - acc: 0.59 - ETA: 8s - loss: 85.9006 - acc: 0.59 - ETA: 7s - loss: 85.6541 - acc: 0.60 - ETA: 6s - loss: 85.4079 - acc: 0.60 - ETA: 6s - loss: 85.1639 - acc: 0.60 - ETA: 5s - loss: 84.9194 - acc: 0.60 - ETA: 4s - loss: 84.6757 - acc: 0.60 - ETA: 3s - loss: 84.4336 - acc: 0.60 - ETA: 3s - loss: 84.1921 - acc: 0.60 - ETA: 2s - loss: 83.9517 - acc: 0.60 - ETA: 1s - loss: 83.7124 - acc: 0.60 - ETA: 1s - loss: 83.4738 - acc: 0.61 - ETA: 0s - loss: 83.2365 - acc: 0.61 - 35s - loss: 83.0647 - acc: 0.6136 - val_loss: 71.4371 - val_acc: 0.6806\n",
      "Epoch 2/40\n",
      "4772/4772 [==============================] - ETA: 32s - loss: 71.4275 - acc: 0.680 - ETA: 30s - loss: 71.2165 - acc: 0.660 - ETA: 29s - loss: 70.9855 - acc: 0.690 - ETA: 29s - loss: 70.7786 - acc: 0.700 - ETA: 29s - loss: 70.5801 - acc: 0.688 - ETA: 28s - loss: 70.3773 - acc: 0.675 - ETA: 28s - loss: 70.1965 - acc: 0.640 - ETA: 27s - loss: 70.0180 - acc: 0.625 - ETA: 26s - loss: 69.8085 - acc: 0.630 - ETA: 25s - loss: 69.5999 - acc: 0.639 - ETA: 24s - loss: 69.3942 - acc: 0.640 - ETA: 24s - loss: 69.1894 - acc: 0.643 - ETA: 23s - loss: 68.9834 - acc: 0.653 - ETA: 22s - loss: 68.7765 - acc: 0.662 - ETA: 21s - loss: 68.5733 - acc: 0.667 - ETA: 21s - loss: 68.3727 - acc: 0.668 - ETA: 20s - loss: 68.1688 - acc: 0.671 - ETA: 19s - loss: 67.9682 - acc: 0.675 - ETA: 19s - loss: 67.7680 - acc: 0.677 - ETA: 18s - loss: 67.5691 - acc: 0.679 - ETA: 17s - loss: 67.3702 - acc: 0.681 - ETA: 17s - loss: 67.1736 - acc: 0.682 - ETA: 16s - loss: 66.9766 - acc: 0.685 - ETA: 15s - loss: 66.7789 - acc: 0.690 - ETA: 15s - loss: 66.5833 - acc: 0.690 - ETA: 14s - loss: 66.3907 - acc: 0.690 - ETA: 13s - loss: 66.2026 - acc: 0.685 - ETA: 13s - loss: 66.0146 - acc: 0.681 - ETA: 12s - loss: 65.8231 - acc: 0.682 - ETA: 11s - loss: 65.6323 - acc: 0.683 - ETA: 11s - loss: 65.4408 - acc: 0.686 - ETA: 10s - loss: 65.2513 - acc: 0.689 - ETA: 9s - loss: 65.0615 - acc: 0.692 - ETA: 9s - loss: 64.8725 - acc: 0.69 - ETA: 8s - loss: 64.6847 - acc: 0.69 - ETA: 7s - loss: 64.5007 - acc: 0.69 - ETA: 7s - loss: 64.3165 - acc: 0.69 - ETA: 6s - loss: 64.1314 - acc: 0.69 - ETA: 5s - loss: 63.9465 - acc: 0.69 - ETA: 5s - loss: 63.7632 - acc: 0.69 - ETA: 4s - loss: 63.5823 - acc: 0.69 - ETA: 3s - loss: 63.4015 - acc: 0.69 - ETA: 3s - loss: 63.2201 - acc: 0.69 - ETA: 2s - loss: 63.0389 - acc: 0.69 - ETA: 1s - loss: 62.8597 - acc: 0.69 - ETA: 1s - loss: 62.6797 - acc: 0.69 - ETA: 0s - loss: 62.5021 - acc: 0.69 - 34s - loss: 62.3736 - acc: 0.6945 - val_loss: 53.6523 - val_acc: 0.7276\n",
      "Epoch 3/40\n",
      "4772/4772 [==============================] - ETA: 32s - loss: 53.5928 - acc: 0.780 - ETA: 30s - loss: 53.4766 - acc: 0.720 - ETA: 29s - loss: 53.3386 - acc: 0.696 - ETA: 28s - loss: 53.1783 - acc: 0.692 - ETA: 28s - loss: 53.0201 - acc: 0.714 - ETA: 27s - loss: 52.8550 - acc: 0.730 - ETA: 26s - loss: 52.6977 - acc: 0.731 - ETA: 26s - loss: 52.5412 - acc: 0.733 - ETA: 25s - loss: 52.3823 - acc: 0.737 - ETA: 24s - loss: 52.2268 - acc: 0.741 - ETA: 23s - loss: 52.0749 - acc: 0.738 - ETA: 23s - loss: 51.9236 - acc: 0.731 - ETA: 22s - loss: 51.7704 - acc: 0.739 - ETA: 21s - loss: 51.6202 - acc: 0.735 - ETA: 21s - loss: 51.4676 - acc: 0.736 - ETA: 20s - loss: 51.3175 - acc: 0.738 - ETA: 20s - loss: 51.1749 - acc: 0.731 - ETA: 19s - loss: 51.0315 - acc: 0.723 - ETA: 18s - loss: 50.8812 - acc: 0.725 - ETA: 18s - loss: 50.7343 - acc: 0.724 - ETA: 17s - loss: 50.5867 - acc: 0.726 - ETA: 16s - loss: 50.4396 - acc: 0.727 - ETA: 16s - loss: 50.2919 - acc: 0.730 - ETA: 15s - loss: 50.1486 - acc: 0.730 - ETA: 14s - loss: 50.0037 - acc: 0.730 - ETA: 14s - loss: 49.8574 - acc: 0.731 - ETA: 13s - loss: 49.7131 - acc: 0.733 - ETA: 12s - loss: 49.5669 - acc: 0.737 - ETA: 12s - loss: 49.4232 - acc: 0.739 - ETA: 11s - loss: 49.2814 - acc: 0.737 - ETA: 10s - loss: 49.1396 - acc: 0.737 - ETA: 10s - loss: 48.9978 - acc: 0.736 - ETA: 9s - loss: 48.8564 - acc: 0.737 - ETA: 8s - loss: 48.7180 - acc: 0.73 - ETA: 8s - loss: 48.5800 - acc: 0.73 - ETA: 7s - loss: 48.4420 - acc: 0.73 - ETA: 6s - loss: 48.3029 - acc: 0.73 - ETA: 6s - loss: 48.1635 - acc: 0.73 - ETA: 5s - loss: 48.0240 - acc: 0.73 - ETA: 4s - loss: 47.8854 - acc: 0.73 - ETA: 4s - loss: 47.7471 - acc: 0.74 - ETA: 3s - loss: 47.6123 - acc: 0.73 - ETA: 3s - loss: 47.4777 - acc: 0.73 - ETA: 2s - loss: 47.3429 - acc: 0.73 - ETA: 1s - loss: 47.2092 - acc: 0.73 - ETA: 1s - loss: 47.0766 - acc: 0.73 - ETA: 0s - loss: 46.9427 - acc: 0.73 - 33s - loss: 46.8467 - acc: 0.7362 - val_loss: 40.3387 - val_acc: 0.7376\n",
      "Epoch 4/40\n",
      "4772/4772 [==============================] - ETA: 32s - loss: 40.2581 - acc: 0.850 - ETA: 30s - loss: 40.1593 - acc: 0.810 - ETA: 29s - loss: 40.0392 - acc: 0.806 - ETA: 28s - loss: 39.9074 - acc: 0.812 - ETA: 27s - loss: 39.8057 - acc: 0.796 - ETA: 26s - loss: 39.7043 - acc: 0.785 - ETA: 26s - loss: 39.5871 - acc: 0.782 - ETA: 25s - loss: 39.4713 - acc: 0.783 - ETA: 25s - loss: 39.3577 - acc: 0.780 - ETA: 24s - loss: 39.2397 - acc: 0.779 - ETA: 23s - loss: 39.1288 - acc: 0.777 - ETA: 23s - loss: 39.0080 - acc: 0.782 - ETA: 22s - loss: 38.9023 - acc: 0.777 - ETA: 22s - loss: 38.7891 - acc: 0.772 - ETA: 21s - loss: 38.6762 - acc: 0.770 - ETA: 20s - loss: 38.5619 - acc: 0.768 - ETA: 20s - loss: 38.4468 - acc: 0.772 - ETA: 19s - loss: 38.3337 - acc: 0.773 - ETA: 18s - loss: 38.2274 - acc: 0.771 - ETA: 18s - loss: 38.1182 - acc: 0.770 - ETA: 17s - loss: 38.0092 - acc: 0.770 - ETA: 16s - loss: 37.8985 - acc: 0.771 - ETA: 16s - loss: 37.7868 - acc: 0.772 - ETA: 15s - loss: 37.6798 - acc: 0.771 - ETA: 14s - loss: 37.5706 - acc: 0.771 - ETA: 14s - loss: 37.4598 - acc: 0.774 - ETA: 13s - loss: 37.3519 - acc: 0.773 - ETA: 12s - loss: 37.2437 - acc: 0.774 - ETA: 12s - loss: 37.1368 - acc: 0.773 - ETA: 11s - loss: 37.0294 - acc: 0.774 - ETA: 10s - loss: 36.9191 - acc: 0.777 - ETA: 10s - loss: 36.8119 - acc: 0.778 - ETA: 9s - loss: 36.7083 - acc: 0.774 - ETA: 8s - loss: 36.6072 - acc: 0.77 - ETA: 8s - loss: 36.5006 - acc: 0.77 - ETA: 7s - loss: 36.3985 - acc: 0.77 - ETA: 6s - loss: 36.2937 - acc: 0.77 - ETA: 6s - loss: 36.1896 - acc: 0.77 - ETA: 5s - loss: 36.0875 - acc: 0.77 - ETA: 5s - loss: 35.9865 - acc: 0.77 - ETA: 4s - loss: 35.8847 - acc: 0.77 - ETA: 3s - loss: 35.7832 - acc: 0.77 - ETA: 3s - loss: 35.6832 - acc: 0.77 - ETA: 2s - loss: 35.5819 - acc: 0.77 - ETA: 1s - loss: 35.4819 - acc: 0.77 - ETA: 1s - loss: 35.3813 - acc: 0.77 - ETA: 0s - loss: 35.2826 - acc: 0.77 - 33s - loss: 35.2110 - acc: 0.7739 - val_loss: 30.3934 - val_acc: 0.7318\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 30s - loss: 30.3706 - acc: 0.770 - ETA: 29s - loss: 30.2669 - acc: 0.740 - ETA: 28s - loss: 30.1726 - acc: 0.730 - ETA: 28s - loss: 30.0664 - acc: 0.750 - ETA: 27s - loss: 29.9649 - acc: 0.764 - ETA: 26s - loss: 29.8733 - acc: 0.770 - ETA: 26s - loss: 29.7862 - acc: 0.771 - ETA: 25s - loss: 29.6999 - acc: 0.767 - ETA: 24s - loss: 29.6146 - acc: 0.765 - ETA: 24s - loss: 29.5345 - acc: 0.762 - ETA: 23s - loss: 29.4466 - acc: 0.766 - ETA: 22s - loss: 29.3555 - acc: 0.770 - ETA: 21s - loss: 29.2763 - acc: 0.764 - ETA: 21s - loss: 29.1906 - acc: 0.767 - ETA: 20s - loss: 29.1028 - acc: 0.770 - ETA: 20s - loss: 29.0197 - acc: 0.771 - ETA: 19s - loss: 28.9385 - acc: 0.769 - ETA: 18s - loss: 28.8576 - acc: 0.768 - ETA: 18s - loss: 28.7738 - acc: 0.768 - ETA: 17s - loss: 28.6888 - acc: 0.771 - ETA: 16s - loss: 28.6008 - acc: 0.775 - ETA: 16s - loss: 28.5199 - acc: 0.772 - ETA: 15s - loss: 28.4371 - acc: 0.773 - ETA: 15s - loss: 28.3534 - acc: 0.774 - ETA: 14s - loss: 28.2753 - acc: 0.773 - ETA: 13s - loss: 28.1961 - acc: 0.773 - ETA: 13s - loss: 28.1170 - acc: 0.771 - ETA: 12s - loss: 28.0366 - acc: 0.772 - ETA: 11s - loss: 27.9574 - acc: 0.772 - ETA: 11s - loss: 27.8758 - acc: 0.774 - ETA: 10s - loss: 27.7951 - acc: 0.775 - ETA: 9s - loss: 27.7135 - acc: 0.777 - ETA: 9s - loss: 27.6344 - acc: 0.77 - ETA: 8s - loss: 27.5570 - acc: 0.77 - ETA: 8s - loss: 27.4798 - acc: 0.77 - ETA: 7s - loss: 27.4010 - acc: 0.77 - ETA: 6s - loss: 27.3225 - acc: 0.77 - ETA: 6s - loss: 27.2437 - acc: 0.77 - ETA: 5s - loss: 27.1667 - acc: 0.77 - ETA: 4s - loss: 27.0921 - acc: 0.77 - ETA: 4s - loss: 27.0162 - acc: 0.77 - ETA: 3s - loss: 26.9380 - acc: 0.77 - ETA: 2s - loss: 26.8616 - acc: 0.77 - ETA: 2s - loss: 26.7848 - acc: 0.77 - ETA: 1s - loss: 26.7118 - acc: 0.77 - ETA: 1s - loss: 26.6390 - acc: 0.77 - ETA: 0s - loss: 26.5650 - acc: 0.77 - 32s - loss: 26.5099 - acc: 0.7735 - val_loss: 22.8933 - val_acc: 0.7443\n",
      "Epoch 6/40\n",
      "4772/4772 [==============================] - ETA: 30s - loss: 22.8470 - acc: 0.800 - ETA: 29s - loss: 22.7963 - acc: 0.765 - ETA: 28s - loss: 22.7187 - acc: 0.780 - ETA: 27s - loss: 22.6369 - acc: 0.790 - ETA: 27s - loss: 22.5691 - acc: 0.786 - ETA: 26s - loss: 22.4987 - acc: 0.790 - ETA: 26s - loss: 22.4286 - acc: 0.792 - ETA: 25s - loss: 22.3637 - acc: 0.791 - ETA: 24s - loss: 22.2940 - acc: 0.790 - ETA: 24s - loss: 22.2356 - acc: 0.790 - ETA: 23s - loss: 22.1682 - acc: 0.790 - ETA: 22s - loss: 22.0991 - acc: 0.794 - ETA: 22s - loss: 22.0336 - acc: 0.797 - ETA: 21s - loss: 21.9720 - acc: 0.797 - ETA: 20s - loss: 21.9081 - acc: 0.799 - ETA: 20s - loss: 21.8427 - acc: 0.801 - ETA: 19s - loss: 21.7749 - acc: 0.802 - ETA: 18s - loss: 21.7102 - acc: 0.804 - ETA: 18s - loss: 21.6534 - acc: 0.799 - ETA: 17s - loss: 21.5912 - acc: 0.799 - ETA: 17s - loss: 21.5308 - acc: 0.798 - ETA: 16s - loss: 21.4709 - acc: 0.796 - ETA: 15s - loss: 21.4102 - acc: 0.795 - ETA: 15s - loss: 21.3517 - acc: 0.794 - ETA: 14s - loss: 21.2910 - acc: 0.792 - ETA: 13s - loss: 21.2311 - acc: 0.790 - ETA: 13s - loss: 21.1686 - acc: 0.792 - ETA: 12s - loss: 21.1091 - acc: 0.793 - ETA: 11s - loss: 21.0489 - acc: 0.795 - ETA: 11s - loss: 20.9884 - acc: 0.796 - ETA: 10s - loss: 20.9291 - acc: 0.796 - ETA: 10s - loss: 20.8694 - acc: 0.796 - ETA: 9s - loss: 20.8104 - acc: 0.795 - ETA: 8s - loss: 20.7506 - acc: 0.79 - ETA: 8s - loss: 20.6951 - acc: 0.79 - ETA: 7s - loss: 20.6388 - acc: 0.79 - ETA: 6s - loss: 20.5798 - acc: 0.79 - ETA: 6s - loss: 20.5205 - acc: 0.79 - ETA: 5s - loss: 20.4621 - acc: 0.79 - ETA: 4s - loss: 20.4058 - acc: 0.79 - ETA: 4s - loss: 20.3470 - acc: 0.79 - ETA: 3s - loss: 20.2924 - acc: 0.79 - ETA: 3s - loss: 20.2350 - acc: 0.79 - ETA: 2s - loss: 20.1784 - acc: 0.79 - ETA: 1s - loss: 20.1243 - acc: 0.79 - ETA: 1s - loss: 20.0701 - acc: 0.78 - ETA: 0s - loss: 20.0146 - acc: 0.78 - 33s - loss: 19.9747 - acc: 0.7892 - val_loss: 17.2980 - val_acc: 0.7552\n",
      "Epoch 7/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 17.2749 - acc: 0.730 - ETA: 29s - loss: 17.1852 - acc: 0.790 - ETA: 28s - loss: 17.1440 - acc: 0.790 - ETA: 27s - loss: 17.1008 - acc: 0.777 - ETA: 27s - loss: 17.0421 - acc: 0.782 - ETA: 27s - loss: 16.9817 - acc: 0.788 - ETA: 27s - loss: 16.9243 - acc: 0.797 - ETA: 26s - loss: 16.8883 - acc: 0.783 - ETA: 26s - loss: 16.8533 - acc: 0.775 - ETA: 25s - loss: 16.7997 - acc: 0.779 - ETA: 25s - loss: 16.7544 - acc: 0.779 - ETA: 24s - loss: 16.7068 - acc: 0.776 - ETA: 23s - loss: 16.6564 - acc: 0.777 - ETA: 22s - loss: 16.6078 - acc: 0.780 - ETA: 22s - loss: 16.5626 - acc: 0.780 - ETA: 21s - loss: 16.5109 - acc: 0.782 - ETA: 20s - loss: 16.4592 - acc: 0.785 - ETA: 19s - loss: 16.4084 - acc: 0.788 - ETA: 19s - loss: 16.3589 - acc: 0.791 - ETA: 18s - loss: 16.3142 - acc: 0.791 - ETA: 17s - loss: 16.2750 - acc: 0.787 - ETA: 16s - loss: 16.2319 - acc: 0.782 - ETA: 16s - loss: 16.1873 - acc: 0.781 - ETA: 15s - loss: 16.1368 - acc: 0.784 - ETA: 14s - loss: 16.0918 - acc: 0.786 - ETA: 14s - loss: 16.0457 - acc: 0.787 - ETA: 13s - loss: 16.0020 - acc: 0.785 - ETA: 12s - loss: 15.9555 - acc: 0.787 - ETA: 12s - loss: 15.9060 - acc: 0.791 - ETA: 11s - loss: 15.8594 - acc: 0.793 - ETA: 10s - loss: 15.8105 - acc: 0.796 - ETA: 10s - loss: 15.7636 - acc: 0.798 - ETA: 9s - loss: 15.7168 - acc: 0.799 - ETA: 8s - loss: 15.6738 - acc: 0.79 - ETA: 8s - loss: 15.6306 - acc: 0.79 - ETA: 7s - loss: 15.5874 - acc: 0.79 - ETA: 6s - loss: 15.5455 - acc: 0.79 - ETA: 6s - loss: 15.5014 - acc: 0.79 - ETA: 5s - loss: 15.4581 - acc: 0.79 - ETA: 4s - loss: 15.4150 - acc: 0.79 - ETA: 4s - loss: 15.3706 - acc: 0.79 - ETA: 3s - loss: 15.3280 - acc: 0.79 - ETA: 3s - loss: 15.2851 - acc: 0.79 - ETA: 2s - loss: 15.2422 - acc: 0.79 - ETA: 1s - loss: 15.1992 - acc: 0.79 - ETA: 1s - loss: 15.1583 - acc: 0.79 - ETA: 0s - loss: 15.1180 - acc: 0.79 - 33s - loss: 15.0874 - acc: 0.7965 - val_loss: 13.0598 - val_acc: 0.7804\n",
      "Epoch 8/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 12.9973 - acc: 0.820 - ETA: 28s - loss: 12.9377 - acc: 0.840 - ETA: 28s - loss: 12.9422 - acc: 0.803 - ETA: 27s - loss: 12.9024 - acc: 0.805 - ETA: 26s - loss: 12.8593 - acc: 0.804 - ETA: 26s - loss: 12.8275 - acc: 0.800 - ETA: 25s - loss: 12.7835 - acc: 0.808 - ETA: 25s - loss: 12.7429 - acc: 0.808 - ETA: 24s - loss: 12.7055 - acc: 0.810 - ETA: 23s - loss: 12.6795 - acc: 0.802 - ETA: 23s - loss: 12.6800 - acc: 0.782 - ETA: 22s - loss: 12.6455 - acc: 0.780 - ETA: 21s - loss: 12.6085 - acc: 0.779 - ETA: 21s - loss: 12.5688 - acc: 0.782 - ETA: 20s - loss: 12.5308 - acc: 0.786 - ETA: 19s - loss: 12.4954 - acc: 0.788 - ETA: 19s - loss: 12.4600 - acc: 0.790 - ETA: 18s - loss: 12.4255 - acc: 0.788 - ETA: 18s - loss: 12.3888 - acc: 0.788 - ETA: 17s - loss: 12.3547 - acc: 0.787 - ETA: 17s - loss: 12.3175 - acc: 0.790 - ETA: 16s - loss: 12.2819 - acc: 0.792 - ETA: 16s - loss: 12.2468 - acc: 0.793 - ETA: 15s - loss: 12.2115 - acc: 0.793 - ETA: 14s - loss: 12.1769 - acc: 0.794 - ETA: 14s - loss: 12.1429 - acc: 0.792 - ETA: 13s - loss: 12.1095 - acc: 0.793 - ETA: 12s - loss: 12.0748 - acc: 0.792 - ETA: 12s - loss: 12.0415 - acc: 0.791 - ETA: 11s - loss: 12.0048 - acc: 0.793 - ETA: 10s - loss: 11.9703 - acc: 0.793 - ETA: 10s - loss: 11.9348 - acc: 0.793 - ETA: 9s - loss: 11.9019 - acc: 0.793 - ETA: 8s - loss: 11.8709 - acc: 0.79 - ETA: 8s - loss: 11.8349 - acc: 0.79 - ETA: 7s - loss: 11.8022 - acc: 0.79 - ETA: 6s - loss: 11.7682 - acc: 0.79 - ETA: 6s - loss: 11.7349 - acc: 0.79 - ETA: 5s - loss: 11.7024 - acc: 0.79 - ETA: 4s - loss: 11.6696 - acc: 0.79 - ETA: 4s - loss: 11.6362 - acc: 0.79 - ETA: 3s - loss: 11.6039 - acc: 0.79 - ETA: 3s - loss: 11.5699 - acc: 0.79 - ETA: 2s - loss: 11.5398 - acc: 0.79 - ETA: 1s - loss: 11.5091 - acc: 0.79 - ETA: 1s - loss: 11.4789 - acc: 0.79 - ETA: 0s - loss: 11.4487 - acc: 0.79 - 33s - loss: 11.4241 - acc: 0.7961 - val_loss: 9.9076 - val_acc: 0.7804\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 29s - loss: 9.8197 - acc: 0.85 - ETA: 29s - loss: 9.8043 - acc: 0.83 - ETA: 29s - loss: 9.8071 - acc: 0.81 - ETA: 29s - loss: 9.7977 - acc: 0.79 - ETA: 28s - loss: 9.7721 - acc: 0.77 - ETA: 28s - loss: 9.7307 - acc: 0.78 - ETA: 27s - loss: 9.6934 - acc: 0.79 - ETA: 27s - loss: 9.6753 - acc: 0.78 - ETA: 26s - loss: 9.6422 - acc: 0.79 - ETA: 25s - loss: 9.6093 - acc: 0.79 - ETA: 24s - loss: 9.5798 - acc: 0.80 - ETA: 23s - loss: 9.5511 - acc: 0.80 - ETA: 23s - loss: 9.5209 - acc: 0.80 - ETA: 22s - loss: 9.5043 - acc: 0.79 - ETA: 21s - loss: 9.4803 - acc: 0.79 - ETA: 21s - loss: 9.4496 - acc: 0.80 - ETA: 20s - loss: 9.4241 - acc: 0.80 - ETA: 19s - loss: 9.3980 - acc: 0.80 - ETA: 18s - loss: 9.3689 - acc: 0.80 - ETA: 18s - loss: 9.3417 - acc: 0.80 - ETA: 17s - loss: 9.3169 - acc: 0.80 - ETA: 16s - loss: 9.2923 - acc: 0.80 - ETA: 16s - loss: 9.2678 - acc: 0.80 - ETA: 15s - loss: 9.2456 - acc: 0.80 - ETA: 14s - loss: 9.2185 - acc: 0.80 - ETA: 14s - loss: 9.1923 - acc: 0.80 - ETA: 13s - loss: 9.1674 - acc: 0.80 - ETA: 12s - loss: 9.1436 - acc: 0.80 - ETA: 12s - loss: 9.1156 - acc: 0.80 - ETA: 11s - loss: 9.0934 - acc: 0.80 - ETA: 10s - loss: 9.0676 - acc: 0.80 - ETA: 10s - loss: 9.0434 - acc: 0.80 - ETA: 9s - loss: 9.0200 - acc: 0.8000 - ETA: 8s - loss: 8.9956 - acc: 0.798 - ETA: 8s - loss: 8.9721 - acc: 0.798 - ETA: 7s - loss: 8.9476 - acc: 0.797 - ETA: 6s - loss: 8.9252 - acc: 0.796 - ETA: 6s - loss: 8.9013 - acc: 0.796 - ETA: 5s - loss: 8.8767 - acc: 0.796 - ETA: 4s - loss: 8.8539 - acc: 0.795 - ETA: 4s - loss: 8.8341 - acc: 0.792 - ETA: 3s - loss: 8.8105 - acc: 0.792 - ETA: 3s - loss: 8.7866 - acc: 0.793 - ETA: 2s - loss: 8.7625 - acc: 0.793 - ETA: 1s - loss: 8.7382 - acc: 0.794 - ETA: 1s - loss: 8.7148 - acc: 0.794 - ETA: 0s - loss: 8.6926 - acc: 0.794 - 33s - loss: 8.6758 - acc: 0.7936 - val_loss: 7.5519 - val_acc: 0.7745\n",
      "Epoch 10/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 7.4493 - acc: 0.84 - ETA: 29s - loss: 7.4699 - acc: 0.81 - ETA: 28s - loss: 7.4246 - acc: 0.84 - ETA: 27s - loss: 7.3985 - acc: 0.84 - ETA: 26s - loss: 7.3737 - acc: 0.85 - ETA: 26s - loss: 7.3565 - acc: 0.85 - ETA: 26s - loss: 7.3375 - acc: 0.84 - ETA: 25s - loss: 7.3266 - acc: 0.83 - ETA: 24s - loss: 7.3392 - acc: 0.82 - ETA: 24s - loss: 7.3299 - acc: 0.81 - ETA: 23s - loss: 7.3098 - acc: 0.81 - ETA: 22s - loss: 7.2865 - acc: 0.81 - ETA: 22s - loss: 7.2645 - acc: 0.82 - ETA: 21s - loss: 7.2422 - acc: 0.82 - ETA: 20s - loss: 7.2277 - acc: 0.81 - ETA: 20s - loss: 7.2073 - acc: 0.82 - ETA: 19s - loss: 7.1875 - acc: 0.82 - ETA: 18s - loss: 7.1737 - acc: 0.81 - ETA: 18s - loss: 7.1520 - acc: 0.81 - ETA: 17s - loss: 7.1330 - acc: 0.81 - ETA: 16s - loss: 7.1108 - acc: 0.82 - ETA: 16s - loss: 7.0933 - acc: 0.81 - ETA: 15s - loss: 7.0734 - acc: 0.81 - ETA: 15s - loss: 7.0527 - acc: 0.81 - ETA: 14s - loss: 7.0323 - acc: 0.81 - ETA: 13s - loss: 7.0096 - acc: 0.81 - ETA: 13s - loss: 6.9904 - acc: 0.81 - ETA: 12s - loss: 6.9693 - acc: 0.81 - ETA: 12s - loss: 6.9511 - acc: 0.81 - ETA: 11s - loss: 6.9325 - acc: 0.81 - ETA: 10s - loss: 6.9123 - acc: 0.81 - ETA: 10s - loss: 6.8928 - acc: 0.81 - ETA: 9s - loss: 6.8724 - acc: 0.8164 - ETA: 8s - loss: 6.8540 - acc: 0.815 - ETA: 8s - loss: 6.8368 - acc: 0.813 - ETA: 7s - loss: 6.8187 - acc: 0.812 - ETA: 7s - loss: 6.8015 - acc: 0.810 - ETA: 6s - loss: 6.7831 - acc: 0.811 - ETA: 5s - loss: 6.7661 - acc: 0.809 - ETA: 5s - loss: 6.7484 - acc: 0.809 - ETA: 4s - loss: 6.7311 - acc: 0.809 - ETA: 3s - loss: 6.7134 - acc: 0.810 - ETA: 3s - loss: 6.6961 - acc: 0.809 - ETA: 2s - loss: 6.6787 - acc: 0.808 - ETA: 1s - loss: 6.6596 - acc: 0.809 - ETA: 1s - loss: 6.6417 - acc: 0.808 - ETA: 0s - loss: 6.6244 - acc: 0.807 - 35s - loss: 6.6109 - acc: 0.8083 - val_loss: 5.8046 - val_acc: 0.7561\n",
      "Epoch 11/40\n",
      "4772/4772 [==============================] - ETA: 33s - loss: 5.7017 - acc: 0.84 - ETA: 33s - loss: 5.6761 - acc: 0.84 - ETA: 32s - loss: 5.6613 - acc: 0.84 - ETA: 31s - loss: 5.6502 - acc: 0.84 - ETA: 30s - loss: 5.6400 - acc: 0.84 - ETA: 29s - loss: 5.6233 - acc: 0.84 - ETA: 28s - loss: 5.6030 - acc: 0.83 - ETA: 27s - loss: 5.5999 - acc: 0.83 - ETA: 26s - loss: 5.5898 - acc: 0.83 - ETA: 25s - loss: 5.5817 - acc: 0.82 - ETA: 25s - loss: 5.5674 - acc: 0.82 - ETA: 24s - loss: 5.5535 - acc: 0.82 - ETA: 23s - loss: 5.5487 - acc: 0.81 - ETA: 22s - loss: 5.5378 - acc: 0.81 - ETA: 22s - loss: 5.5224 - acc: 0.81 - ETA: 21s - loss: 5.5031 - acc: 0.81 - ETA: 20s - loss: 5.4886 - acc: 0.81 - ETA: 20s - loss: 5.4758 - acc: 0.81 - ETA: 19s - loss: 5.4573 - acc: 0.81 - ETA: 19s - loss: 5.4423 - acc: 0.81 - ETA: 18s - loss: 5.4251 - acc: 0.81 - ETA: 17s - loss: 5.4060 - acc: 0.82 - ETA: 17s - loss: 5.3925 - acc: 0.82 - ETA: 16s - loss: 5.3767 - acc: 0.82 - ETA: 15s - loss: 5.3657 - acc: 0.81 - ETA: 15s - loss: 5.3538 - acc: 0.81 - ETA: 14s - loss: 5.3423 - acc: 0.81 - ETA: 13s - loss: 5.3311 - acc: 0.81 - ETA: 13s - loss: 5.3180 - acc: 0.81 - ETA: 12s - loss: 5.3017 - acc: 0.81 - ETA: 11s - loss: 5.2857 - acc: 0.81 - ETA: 11s - loss: 5.2722 - acc: 0.81 - ETA: 10s - loss: 5.2584 - acc: 0.81 - ETA: 9s - loss: 5.2445 - acc: 0.8129 - ETA: 8s - loss: 5.2315 - acc: 0.813 - ETA: 8s - loss: 5.2207 - acc: 0.811 - ETA: 7s - loss: 5.2067 - acc: 0.811 - ETA: 6s - loss: 5.1919 - acc: 0.812 - ETA: 6s - loss: 5.1785 - acc: 0.812 - ETA: 5s - loss: 5.1642 - acc: 0.813 - ETA: 4s - loss: 5.1511 - acc: 0.812 - ETA: 3s - loss: 5.1376 - acc: 0.813 - ETA: 3s - loss: 5.1238 - acc: 0.814 - ETA: 2s - loss: 5.1099 - acc: 0.814 - ETA: 1s - loss: 5.0970 - acc: 0.813 - ETA: 1s - loss: 5.0851 - acc: 0.812 - ETA: 0s - loss: 5.0712 - acc: 0.813 - 36s - loss: 5.0620 - acc: 0.8122 - val_loss: 4.4437 - val_acc: 0.7846\n",
      "Epoch 12/40\n",
      "4772/4772 [==============================] - ETA: 33s - loss: 4.3927 - acc: 0.84 - ETA: 34s - loss: 4.4369 - acc: 0.78 - ETA: 34s - loss: 4.4161 - acc: 0.79 - ETA: 34s - loss: 4.3903 - acc: 0.80 - ETA: 34s - loss: 4.3860 - acc: 0.79 - ETA: 33s - loss: 4.3527 - acc: 0.81 - ETA: 32s - loss: 4.3304 - acc: 0.82 - ETA: 31s - loss: 4.3254 - acc: 0.81 - ETA: 30s - loss: 4.3206 - acc: 0.80 - ETA: 29s - loss: 4.3112 - acc: 0.80 - ETA: 28s - loss: 4.2901 - acc: 0.81 - ETA: 27s - loss: 4.2794 - acc: 0.81 - ETA: 26s - loss: 4.2746 - acc: 0.81 - ETA: 25s - loss: 4.2606 - acc: 0.81 - ETA: 24s - loss: 4.2503 - acc: 0.81 - ETA: 23s - loss: 4.2409 - acc: 0.81 - ETA: 22s - loss: 4.2304 - acc: 0.80 - ETA: 22s - loss: 4.2218 - acc: 0.80 - ETA: 21s - loss: 4.2124 - acc: 0.80 - ETA: 20s - loss: 4.2011 - acc: 0.80 - ETA: 20s - loss: 4.1894 - acc: 0.80 - ETA: 19s - loss: 4.1761 - acc: 0.80 - ETA: 18s - loss: 4.1632 - acc: 0.80 - ETA: 17s - loss: 4.1492 - acc: 0.81 - ETA: 17s - loss: 4.1411 - acc: 0.80 - ETA: 16s - loss: 4.1318 - acc: 0.80 - ETA: 15s - loss: 4.1206 - acc: 0.80 - ETA: 14s - loss: 4.1097 - acc: 0.81 - ETA: 14s - loss: 4.0974 - acc: 0.81 - ETA: 13s - loss: 4.0832 - acc: 0.81 - ETA: 12s - loss: 4.0735 - acc: 0.81 - ETA: 11s - loss: 4.0616 - acc: 0.81 - ETA: 10s - loss: 4.0508 - acc: 0.81 - ETA: 10s - loss: 4.0407 - acc: 0.81 - ETA: 9s - loss: 4.0331 - acc: 0.8109 - ETA: 8s - loss: 4.0262 - acc: 0.807 - ETA: 7s - loss: 4.0154 - acc: 0.808 - ETA: 7s - loss: 4.0030 - acc: 0.809 - ETA: 6s - loss: 3.9937 - acc: 0.809 - ETA: 5s - loss: 3.9826 - acc: 0.810 - ETA: 4s - loss: 3.9702 - acc: 0.812 - ETA: 4s - loss: 3.9597 - acc: 0.814 - ETA: 3s - loss: 3.9507 - acc: 0.813 - ETA: 2s - loss: 3.9415 - acc: 0.813 - ETA: 1s - loss: 3.9313 - acc: 0.813 - ETA: 1s - loss: 3.9203 - acc: 0.814 - ETA: 0s - loss: 3.9096 - acc: 0.814 - 37s - loss: 3.9028 - acc: 0.8139 - val_loss: 3.4460 - val_acc: 0.7846\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 33s - loss: 3.4540 - acc: 0.84 - ETA: 32s - loss: 3.4331 - acc: 0.83 - ETA: 31s - loss: 3.4022 - acc: 0.83 - ETA: 31s - loss: 3.3728 - acc: 0.83 - ETA: 30s - loss: 3.3450 - acc: 0.84 - ETA: 29s - loss: 3.3459 - acc: 0.84 - ETA: 29s - loss: 3.3367 - acc: 0.84 - ETA: 28s - loss: 3.3237 - acc: 0.84 - ETA: 27s - loss: 3.3135 - acc: 0.84 - ETA: 26s - loss: 3.3046 - acc: 0.84 - ETA: 25s - loss: 3.3024 - acc: 0.83 - ETA: 25s - loss: 3.2945 - acc: 0.83 - ETA: 24s - loss: 3.2902 - acc: 0.83 - ETA: 23s - loss: 3.2894 - acc: 0.82 - ETA: 22s - loss: 3.2870 - acc: 0.82 - ETA: 21s - loss: 3.2809 - acc: 0.82 - ETA: 21s - loss: 3.2716 - acc: 0.82 - ETA: 20s - loss: 3.2634 - acc: 0.81 - ETA: 19s - loss: 3.2544 - acc: 0.82 - ETA: 18s - loss: 3.2493 - acc: 0.81 - ETA: 18s - loss: 3.2384 - acc: 0.81 - ETA: 17s - loss: 3.2311 - acc: 0.81 - ETA: 16s - loss: 3.2220 - acc: 0.81 - ETA: 16s - loss: 3.2125 - acc: 0.82 - ETA: 15s - loss: 3.2045 - acc: 0.81 - ETA: 14s - loss: 3.2008 - acc: 0.81 - ETA: 13s - loss: 3.1944 - acc: 0.81 - ETA: 13s - loss: 3.1852 - acc: 0.81 - ETA: 12s - loss: 3.1781 - acc: 0.81 - ETA: 12s - loss: 3.1667 - acc: 0.81 - ETA: 11s - loss: 3.1604 - acc: 0.81 - ETA: 10s - loss: 3.1498 - acc: 0.81 - ETA: 10s - loss: 3.1416 - acc: 0.81 - ETA: 9s - loss: 3.1332 - acc: 0.8200 - ETA: 8s - loss: 3.1252 - acc: 0.819 - ETA: 8s - loss: 3.1181 - acc: 0.819 - ETA: 7s - loss: 3.1101 - acc: 0.820 - ETA: 6s - loss: 3.1002 - acc: 0.821 - ETA: 6s - loss: 3.0926 - acc: 0.821 - ETA: 5s - loss: 3.0835 - acc: 0.822 - ETA: 4s - loss: 3.0756 - acc: 0.822 - ETA: 3s - loss: 3.0686 - acc: 0.822 - ETA: 3s - loss: 3.0627 - acc: 0.821 - ETA: 2s - loss: 3.0555 - acc: 0.820 - ETA: 1s - loss: 3.0474 - acc: 0.820 - ETA: 1s - loss: 3.0405 - acc: 0.820 - ETA: 0s - loss: 3.0322 - acc: 0.820 - 36s - loss: 3.0281 - acc: 0.8198 - val_loss: 2.7064 - val_acc: 0.7946\n",
      "Epoch 14/40\n",
      "4772/4772 [==============================] - ETA: 31s - loss: 2.6009 - acc: 0.87 - ETA: 30s - loss: 2.5734 - acc: 0.88 - ETA: 29s - loss: 2.5857 - acc: 0.86 - ETA: 30s - loss: 2.6076 - acc: 0.84 - ETA: 30s - loss: 2.6124 - acc: 0.83 - ETA: 29s - loss: 2.6191 - acc: 0.82 - ETA: 29s - loss: 2.6139 - acc: 0.82 - ETA: 28s - loss: 2.6113 - acc: 0.81 - ETA: 27s - loss: 2.6206 - acc: 0.80 - ETA: 27s - loss: 2.6209 - acc: 0.80 - ETA: 26s - loss: 2.6119 - acc: 0.80 - ETA: 25s - loss: 2.6058 - acc: 0.80 - ETA: 24s - loss: 2.5948 - acc: 0.81 - ETA: 24s - loss: 2.5940 - acc: 0.80 - ETA: 23s - loss: 2.5920 - acc: 0.80 - ETA: 22s - loss: 2.5876 - acc: 0.80 - ETA: 22s - loss: 2.5776 - acc: 0.80 - ETA: 21s - loss: 2.5680 - acc: 0.80 - ETA: 20s - loss: 2.5676 - acc: 0.79 - ETA: 19s - loss: 2.5615 - acc: 0.80 - ETA: 19s - loss: 2.5561 - acc: 0.79 - ETA: 18s - loss: 2.5501 - acc: 0.79 - ETA: 17s - loss: 2.5417 - acc: 0.80 - ETA: 17s - loss: 2.5341 - acc: 0.80 - ETA: 16s - loss: 2.5251 - acc: 0.80 - ETA: 15s - loss: 2.5189 - acc: 0.80 - ETA: 14s - loss: 2.5090 - acc: 0.80 - ETA: 13s - loss: 2.5053 - acc: 0.80 - ETA: 13s - loss: 2.4970 - acc: 0.80 - ETA: 12s - loss: 2.4920 - acc: 0.80 - ETA: 11s - loss: 2.4862 - acc: 0.80 - ETA: 11s - loss: 2.4797 - acc: 0.80 - ETA: 10s - loss: 2.4715 - acc: 0.80 - ETA: 9s - loss: 2.4665 - acc: 0.8079 - ETA: 9s - loss: 2.4580 - acc: 0.809 - ETA: 8s - loss: 2.4515 - acc: 0.809 - ETA: 7s - loss: 2.4448 - acc: 0.810 - ETA: 6s - loss: 2.4409 - acc: 0.809 - ETA: 6s - loss: 2.4375 - acc: 0.808 - ETA: 5s - loss: 2.4311 - acc: 0.808 - ETA: 4s - loss: 2.4245 - acc: 0.809 - ETA: 4s - loss: 2.4189 - acc: 0.809 - ETA: 3s - loss: 2.4117 - acc: 0.810 - ETA: 2s - loss: 2.4051 - acc: 0.811 - ETA: 1s - loss: 2.3980 - acc: 0.812 - ETA: 1s - loss: 2.3915 - acc: 0.813 - ETA: 0s - loss: 2.3856 - acc: 0.814 - 36s - loss: 2.3817 - acc: 0.8139 - val_loss: 2.1815 - val_acc: 0.7737\n",
      "Epoch 15/40\n",
      "4772/4772 [==============================] - ETA: 31s - loss: 2.1693 - acc: 0.80 - ETA: 30s - loss: 2.1868 - acc: 0.78 - ETA: 29s - loss: 2.1696 - acc: 0.79 - ETA: 30s - loss: 2.1521 - acc: 0.80 - ETA: 30s - loss: 2.1347 - acc: 0.80 - ETA: 29s - loss: 2.1192 - acc: 0.81 - ETA: 29s - loss: 2.0981 - acc: 0.81 - ETA: 28s - loss: 2.0862 - acc: 0.82 - ETA: 27s - loss: 2.0790 - acc: 0.82 - ETA: 27s - loss: 2.0710 - acc: 0.82 - ETA: 26s - loss: 2.0627 - acc: 0.82 - ETA: 25s - loss: 2.0524 - acc: 0.82 - ETA: 24s - loss: 2.0505 - acc: 0.81 - ETA: 23s - loss: 2.0500 - acc: 0.81 - ETA: 22s - loss: 2.0429 - acc: 0.81 - ETA: 22s - loss: 2.0397 - acc: 0.81 - ETA: 21s - loss: 2.0332 - acc: 0.81 - ETA: 20s - loss: 2.0284 - acc: 0.81 - ETA: 20s - loss: 2.0227 - acc: 0.81 - ETA: 19s - loss: 2.0156 - acc: 0.81 - ETA: 18s - loss: 2.0073 - acc: 0.82 - ETA: 17s - loss: 2.0009 - acc: 0.82 - ETA: 17s - loss: 1.9922 - acc: 0.82 - ETA: 16s - loss: 1.9841 - acc: 0.82 - ETA: 15s - loss: 1.9797 - acc: 0.82 - ETA: 14s - loss: 1.9749 - acc: 0.82 - ETA: 14s - loss: 1.9674 - acc: 0.82 - ETA: 13s - loss: 1.9631 - acc: 0.82 - ETA: 12s - loss: 1.9563 - acc: 0.82 - ETA: 12s - loss: 1.9480 - acc: 0.83 - ETA: 11s - loss: 1.9423 - acc: 0.83 - ETA: 10s - loss: 1.9413 - acc: 0.82 - ETA: 10s - loss: 1.9396 - acc: 0.82 - ETA: 9s - loss: 1.9334 - acc: 0.8300 - ETA: 8s - loss: 1.9294 - acc: 0.830 - ETA: 8s - loss: 1.9249 - acc: 0.830 - ETA: 7s - loss: 1.9213 - acc: 0.830 - ETA: 6s - loss: 1.9207 - acc: 0.828 - ETA: 6s - loss: 1.9173 - acc: 0.826 - ETA: 5s - loss: 1.9142 - acc: 0.826 - ETA: 4s - loss: 1.9114 - acc: 0.824 - ETA: 4s - loss: 1.9071 - acc: 0.824 - ETA: 3s - loss: 1.9027 - acc: 0.824 - ETA: 2s - loss: 1.9001 - acc: 0.822 - ETA: 1s - loss: 1.8948 - acc: 0.824 - ETA: 1s - loss: 1.8913 - acc: 0.823 - ETA: 0s - loss: 1.8884 - acc: 0.822 - 36s - loss: 1.8853 - acc: 0.8227 - val_loss: 1.7157 - val_acc: 0.8089\n",
      "Epoch 16/40\n",
      "4772/4772 [==============================] - ETA: 32s - loss: 1.5706 - acc: 0.90 - ETA: 31s - loss: 1.6089 - acc: 0.86 - ETA: 30s - loss: 1.6261 - acc: 0.84 - ETA: 29s - loss: 1.6307 - acc: 0.85 - ETA: 29s - loss: 1.6349 - acc: 0.84 - ETA: 28s - loss: 1.6393 - acc: 0.84 - ETA: 27s - loss: 1.6343 - acc: 0.84 - ETA: 26s - loss: 1.6442 - acc: 0.83 - ETA: 25s - loss: 1.6503 - acc: 0.83 - ETA: 25s - loss: 1.6420 - acc: 0.83 - ETA: 24s - loss: 1.6345 - acc: 0.83 - ETA: 23s - loss: 1.6483 - acc: 0.82 - ETA: 22s - loss: 1.6448 - acc: 0.82 - ETA: 22s - loss: 1.6439 - acc: 0.82 - ETA: 21s - loss: 1.6392 - acc: 0.82 - ETA: 20s - loss: 1.6377 - acc: 0.82 - ETA: 20s - loss: 1.6306 - acc: 0.82 - ETA: 19s - loss: 1.6243 - acc: 0.82 - ETA: 18s - loss: 1.6180 - acc: 0.83 - ETA: 18s - loss: 1.6125 - acc: 0.83 - ETA: 17s - loss: 1.6071 - acc: 0.83 - ETA: 17s - loss: 1.6043 - acc: 0.83 - ETA: 16s - loss: 1.5956 - acc: 0.83 - ETA: 15s - loss: 1.5945 - acc: 0.83 - ETA: 15s - loss: 1.5914 - acc: 0.83 - ETA: 14s - loss: 1.5870 - acc: 0.83 - ETA: 13s - loss: 1.5861 - acc: 0.83 - ETA: 12s - loss: 1.5796 - acc: 0.83 - ETA: 12s - loss: 1.5818 - acc: 0.83 - ETA: 11s - loss: 1.5788 - acc: 0.83 - ETA: 11s - loss: 1.5748 - acc: 0.83 - ETA: 10s - loss: 1.5716 - acc: 0.83 - ETA: 9s - loss: 1.5707 - acc: 0.8285 - ETA: 9s - loss: 1.5673 - acc: 0.828 - ETA: 8s - loss: 1.5642 - acc: 0.827 - ETA: 7s - loss: 1.5605 - acc: 0.826 - ETA: 7s - loss: 1.5562 - acc: 0.827 - ETA: 6s - loss: 1.5520 - acc: 0.827 - ETA: 5s - loss: 1.5461 - acc: 0.829 - ETA: 5s - loss: 1.5434 - acc: 0.828 - ETA: 4s - loss: 1.5396 - acc: 0.829 - ETA: 3s - loss: 1.5369 - acc: 0.828 - ETA: 3s - loss: 1.5366 - acc: 0.827 - ETA: 2s - loss: 1.5351 - acc: 0.826 - ETA: 1s - loss: 1.5329 - acc: 0.826 - ETA: 1s - loss: 1.5297 - acc: 0.826 - ETA: 0s - loss: 1.5261 - acc: 0.827 - 33s - loss: 1.5235 - acc: 0.8269 - val_loss: 1.4132 - val_acc: 0.7913\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 31s - loss: 1.3638 - acc: 0.83 - ETA: 30s - loss: 1.3671 - acc: 0.83 - ETA: 29s - loss: 1.3426 - acc: 0.84 - ETA: 28s - loss: 1.3247 - acc: 0.85 - ETA: 28s - loss: 1.3178 - acc: 0.85 - ETA: 27s - loss: 1.3373 - acc: 0.83 - ETA: 26s - loss: 1.3578 - acc: 0.82 - ETA: 25s - loss: 1.3579 - acc: 0.82 - ETA: 25s - loss: 1.3488 - acc: 0.82 - ETA: 24s - loss: 1.3511 - acc: 0.82 - ETA: 23s - loss: 1.3562 - acc: 0.81 - ETA: 23s - loss: 1.3602 - acc: 0.81 - ETA: 22s - loss: 1.3530 - acc: 0.81 - ETA: 21s - loss: 1.3420 - acc: 0.82 - ETA: 21s - loss: 1.3383 - acc: 0.82 - ETA: 20s - loss: 1.3374 - acc: 0.82 - ETA: 19s - loss: 1.3315 - acc: 0.82 - ETA: 19s - loss: 1.3280 - acc: 0.82 - ETA: 18s - loss: 1.3205 - acc: 0.82 - ETA: 17s - loss: 1.3205 - acc: 0.82 - ETA: 17s - loss: 1.3158 - acc: 0.82 - ETA: 16s - loss: 1.3100 - acc: 0.82 - ETA: 15s - loss: 1.3047 - acc: 0.83 - ETA: 15s - loss: 1.2981 - acc: 0.83 - ETA: 14s - loss: 1.2935 - acc: 0.83 - ETA: 13s - loss: 1.2903 - acc: 0.83 - ETA: 13s - loss: 1.2854 - acc: 0.83 - ETA: 12s - loss: 1.2814 - acc: 0.83 - ETA: 12s - loss: 1.2828 - acc: 0.83 - ETA: 11s - loss: 1.2851 - acc: 0.83 - ETA: 10s - loss: 1.2847 - acc: 0.83 - ETA: 10s - loss: 1.2824 - acc: 0.83 - ETA: 9s - loss: 1.2793 - acc: 0.8321 - ETA: 8s - loss: 1.2816 - acc: 0.828 - ETA: 8s - loss: 1.2795 - acc: 0.828 - ETA: 7s - loss: 1.2773 - acc: 0.828 - ETA: 6s - loss: 1.2750 - acc: 0.828 - ETA: 6s - loss: 1.2712 - acc: 0.829 - ETA: 5s - loss: 1.2687 - acc: 0.829 - ETA: 4s - loss: 1.2664 - acc: 0.827 - ETA: 4s - loss: 1.2644 - acc: 0.827 - ETA: 3s - loss: 1.2620 - acc: 0.826 - ETA: 3s - loss: 1.2586 - acc: 0.827 - ETA: 2s - loss: 1.2554 - acc: 0.828 - ETA: 1s - loss: 1.2525 - acc: 0.828 - ETA: 1s - loss: 1.2507 - acc: 0.828 - ETA: 0s - loss: 1.2476 - acc: 0.829 - 33s - loss: 1.2447 - acc: 0.8301 - val_loss: 1.3046 - val_acc: 0.7427\n",
      "Epoch 18/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 1.2038 - acc: 0.80 - ETA: 29s - loss: 1.1740 - acc: 0.81 - ETA: 28s - loss: 1.1361 - acc: 0.82 - ETA: 28s - loss: 1.1330 - acc: 0.83 - ETA: 27s - loss: 1.1213 - acc: 0.83 - ETA: 26s - loss: 1.1100 - acc: 0.83 - ETA: 26s - loss: 1.1063 - acc: 0.84 - ETA: 25s - loss: 1.1156 - acc: 0.83 - ETA: 24s - loss: 1.1066 - acc: 0.84 - ETA: 24s - loss: 1.1021 - acc: 0.83 - ETA: 23s - loss: 1.0965 - acc: 0.84 - ETA: 22s - loss: 1.0947 - acc: 0.84 - ETA: 22s - loss: 1.0895 - acc: 0.84 - ETA: 21s - loss: 1.0935 - acc: 0.83 - ETA: 20s - loss: 1.0934 - acc: 0.83 - ETA: 20s - loss: 1.0936 - acc: 0.83 - ETA: 19s - loss: 1.0916 - acc: 0.83 - ETA: 19s - loss: 1.0919 - acc: 0.83 - ETA: 18s - loss: 1.0876 - acc: 0.83 - ETA: 17s - loss: 1.0812 - acc: 0.84 - ETA: 17s - loss: 1.0809 - acc: 0.83 - ETA: 16s - loss: 1.0780 - acc: 0.83 - ETA: 15s - loss: 1.0753 - acc: 0.83 - ETA: 15s - loss: 1.0723 - acc: 0.84 - ETA: 14s - loss: 1.0703 - acc: 0.84 - ETA: 14s - loss: 1.0724 - acc: 0.83 - ETA: 13s - loss: 1.0740 - acc: 0.83 - ETA: 12s - loss: 1.0707 - acc: 0.83 - ETA: 12s - loss: 1.0664 - acc: 0.84 - ETA: 11s - loss: 1.0633 - acc: 0.84 - ETA: 10s - loss: 1.0600 - acc: 0.84 - ETA: 10s - loss: 1.0566 - acc: 0.84 - ETA: 9s - loss: 1.0524 - acc: 0.8445 - ETA: 8s - loss: 1.0501 - acc: 0.845 - ETA: 8s - loss: 1.0487 - acc: 0.845 - ETA: 7s - loss: 1.0458 - acc: 0.845 - ETA: 6s - loss: 1.0478 - acc: 0.842 - ETA: 6s - loss: 1.0491 - acc: 0.840 - ETA: 5s - loss: 1.0470 - acc: 0.839 - ETA: 4s - loss: 1.0448 - acc: 0.841 - ETA: 4s - loss: 1.0443 - acc: 0.840 - ETA: 3s - loss: 1.0407 - acc: 0.841 - ETA: 3s - loss: 1.0372 - acc: 0.842 - ETA: 2s - loss: 1.0341 - acc: 0.843 - ETA: 1s - loss: 1.0336 - acc: 0.843 - ETA: 1s - loss: 1.0352 - acc: 0.841 - ETA: 0s - loss: 1.0353 - acc: 0.840 - 33s - loss: 1.0353 - acc: 0.8384 - val_loss: 1.0113 - val_acc: 0.7888\n",
      "Epoch 19/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.9192 - acc: 0.84 - ETA: 29s - loss: 0.8889 - acc: 0.88 - ETA: 28s - loss: 0.9143 - acc: 0.87 - ETA: 28s - loss: 0.9115 - acc: 0.86 - ETA: 27s - loss: 0.9093 - acc: 0.86 - ETA: 26s - loss: 0.8947 - acc: 0.86 - ETA: 26s - loss: 0.8886 - acc: 0.86 - ETA: 25s - loss: 0.8970 - acc: 0.86 - ETA: 24s - loss: 0.8935 - acc: 0.86 - ETA: 24s - loss: 0.8948 - acc: 0.85 - ETA: 23s - loss: 0.8924 - acc: 0.85 - ETA: 23s - loss: 0.8963 - acc: 0.85 - ETA: 22s - loss: 0.9000 - acc: 0.85 - ETA: 22s - loss: 0.8991 - acc: 0.85 - ETA: 21s - loss: 0.8950 - acc: 0.85 - ETA: 20s - loss: 0.8907 - acc: 0.86 - ETA: 20s - loss: 0.8841 - acc: 0.86 - ETA: 19s - loss: 0.8818 - acc: 0.86 - ETA: 18s - loss: 0.8837 - acc: 0.85 - ETA: 18s - loss: 0.8873 - acc: 0.85 - ETA: 17s - loss: 0.8916 - acc: 0.85 - ETA: 16s - loss: 0.8972 - acc: 0.84 - ETA: 16s - loss: 0.8943 - acc: 0.85 - ETA: 15s - loss: 0.8939 - acc: 0.84 - ETA: 14s - loss: 0.8919 - acc: 0.84 - ETA: 14s - loss: 0.8902 - acc: 0.84 - ETA: 13s - loss: 0.8857 - acc: 0.85 - ETA: 12s - loss: 0.8866 - acc: 0.85 - ETA: 12s - loss: 0.8884 - acc: 0.84 - ETA: 11s - loss: 0.8879 - acc: 0.84 - ETA: 10s - loss: 0.8867 - acc: 0.84 - ETA: 10s - loss: 0.8870 - acc: 0.84 - ETA: 9s - loss: 0.8877 - acc: 0.8458 - ETA: 8s - loss: 0.8905 - acc: 0.842 - ETA: 8s - loss: 0.8919 - acc: 0.840 - ETA: 7s - loss: 0.8903 - acc: 0.841 - ETA: 6s - loss: 0.8893 - acc: 0.841 - ETA: 6s - loss: 0.8885 - acc: 0.840 - ETA: 5s - loss: 0.8885 - acc: 0.839 - ETA: 5s - loss: 0.8874 - acc: 0.838 - ETA: 4s - loss: 0.8858 - acc: 0.838 - ETA: 3s - loss: 0.8847 - acc: 0.838 - ETA: 3s - loss: 0.8821 - acc: 0.839 - ETA: 2s - loss: 0.8804 - acc: 0.839 - ETA: 1s - loss: 0.8779 - acc: 0.840 - ETA: 1s - loss: 0.8757 - acc: 0.840 - ETA: 0s - loss: 0.8740 - acc: 0.840 - 33s - loss: 0.8728 - acc: 0.8403 - val_loss: 0.8604 - val_acc: 0.7997\n",
      "Epoch 20/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.7362 - acc: 0.90 - ETA: 29s - loss: 0.7825 - acc: 0.86 - ETA: 28s - loss: 0.7688 - acc: 0.88 - ETA: 27s - loss: 0.7566 - acc: 0.89 - ETA: 27s - loss: 0.7616 - acc: 0.88 - ETA: 26s - loss: 0.7551 - acc: 0.88 - ETA: 25s - loss: 0.7642 - acc: 0.87 - ETA: 25s - loss: 0.7776 - acc: 0.86 - ETA: 24s - loss: 0.7851 - acc: 0.85 - ETA: 24s - loss: 0.7860 - acc: 0.85 - ETA: 23s - loss: 0.7833 - acc: 0.86 - ETA: 22s - loss: 0.7853 - acc: 0.86 - ETA: 22s - loss: 0.7816 - acc: 0.86 - ETA: 21s - loss: 0.7823 - acc: 0.85 - ETA: 20s - loss: 0.7832 - acc: 0.85 - ETA: 20s - loss: 0.7789 - acc: 0.85 - ETA: 19s - loss: 0.7814 - acc: 0.85 - ETA: 19s - loss: 0.7875 - acc: 0.84 - ETA: 18s - loss: 0.7849 - acc: 0.84 - ETA: 17s - loss: 0.7846 - acc: 0.84 - ETA: 17s - loss: 0.7833 - acc: 0.84 - ETA: 16s - loss: 0.7806 - acc: 0.85 - ETA: 15s - loss: 0.7768 - acc: 0.85 - ETA: 15s - loss: 0.7735 - acc: 0.85 - ETA: 14s - loss: 0.7731 - acc: 0.85 - ETA: 14s - loss: 0.7710 - acc: 0.84 - ETA: 13s - loss: 0.7716 - acc: 0.84 - ETA: 12s - loss: 0.7709 - acc: 0.84 - ETA: 12s - loss: 0.7690 - acc: 0.85 - ETA: 11s - loss: 0.7650 - acc: 0.85 - ETA: 10s - loss: 0.7615 - acc: 0.85 - ETA: 10s - loss: 0.7637 - acc: 0.84 - ETA: 9s - loss: 0.7625 - acc: 0.8497 - ETA: 8s - loss: 0.7650 - acc: 0.847 - ETA: 8s - loss: 0.7670 - acc: 0.845 - ETA: 7s - loss: 0.7701 - acc: 0.842 - ETA: 6s - loss: 0.7690 - acc: 0.841 - ETA: 6s - loss: 0.7672 - acc: 0.841 - ETA: 5s - loss: 0.7644 - acc: 0.843 - ETA: 5s - loss: 0.7631 - acc: 0.843 - ETA: 4s - loss: 0.7618 - acc: 0.843 - ETA: 3s - loss: 0.7609 - acc: 0.843 - ETA: 3s - loss: 0.7616 - acc: 0.843 - ETA: 2s - loss: 0.7588 - acc: 0.844 - ETA: 1s - loss: 0.7572 - acc: 0.845 - ETA: 1s - loss: 0.7553 - acc: 0.845 - ETA: 0s - loss: 0.7539 - acc: 0.845 - 34s - loss: 0.7546 - acc: 0.8445 - val_loss: 0.8305 - val_acc: 0.7661\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 29s - loss: 0.7683 - acc: 0.79 - ETA: 29s - loss: 0.7274 - acc: 0.82 - ETA: 29s - loss: 0.6865 - acc: 0.85 - ETA: 29s - loss: 0.6891 - acc: 0.84 - ETA: 28s - loss: 0.7050 - acc: 0.84 - ETA: 27s - loss: 0.7033 - acc: 0.84 - ETA: 27s - loss: 0.7000 - acc: 0.84 - ETA: 26s - loss: 0.6939 - acc: 0.84 - ETA: 26s - loss: 0.6860 - acc: 0.84 - ETA: 25s - loss: 0.6836 - acc: 0.85 - ETA: 24s - loss: 0.6775 - acc: 0.85 - ETA: 24s - loss: 0.6846 - acc: 0.84 - ETA: 23s - loss: 0.6880 - acc: 0.84 - ETA: 22s - loss: 0.6840 - acc: 0.84 - ETA: 22s - loss: 0.6830 - acc: 0.84 - ETA: 21s - loss: 0.6780 - acc: 0.84 - ETA: 20s - loss: 0.6755 - acc: 0.85 - ETA: 19s - loss: 0.6748 - acc: 0.85 - ETA: 19s - loss: 0.6753 - acc: 0.84 - ETA: 18s - loss: 0.6798 - acc: 0.84 - ETA: 17s - loss: 0.6811 - acc: 0.84 - ETA: 17s - loss: 0.6791 - acc: 0.84 - ETA: 16s - loss: 0.6781 - acc: 0.84 - ETA: 15s - loss: 0.6762 - acc: 0.84 - ETA: 15s - loss: 0.6788 - acc: 0.84 - ETA: 14s - loss: 0.6773 - acc: 0.84 - ETA: 14s - loss: 0.6761 - acc: 0.84 - ETA: 13s - loss: 0.6757 - acc: 0.84 - ETA: 12s - loss: 0.6749 - acc: 0.84 - ETA: 12s - loss: 0.6740 - acc: 0.84 - ETA: 11s - loss: 0.6706 - acc: 0.84 - ETA: 10s - loss: 0.6676 - acc: 0.84 - ETA: 9s - loss: 0.6667 - acc: 0.8482 - ETA: 9s - loss: 0.6641 - acc: 0.849 - ETA: 8s - loss: 0.6628 - acc: 0.850 - ETA: 7s - loss: 0.6637 - acc: 0.848 - ETA: 7s - loss: 0.6649 - acc: 0.847 - ETA: 6s - loss: 0.6633 - acc: 0.848 - ETA: 5s - loss: 0.6620 - acc: 0.848 - ETA: 5s - loss: 0.6605 - acc: 0.849 - ETA: 4s - loss: 0.6613 - acc: 0.849 - ETA: 3s - loss: 0.6629 - acc: 0.847 - ETA: 3s - loss: 0.6639 - acc: 0.846 - ETA: 2s - loss: 0.6638 - acc: 0.846 - ETA: 1s - loss: 0.6623 - acc: 0.846 - ETA: 1s - loss: 0.6610 - acc: 0.847 - ETA: 0s - loss: 0.6595 - acc: 0.847 - 34s - loss: 0.6605 - acc: 0.8468 - val_loss: 0.7586 - val_acc: 0.7678\n",
      "Epoch 22/40\n",
      "4772/4772 [==============================] - ETA: 31s - loss: 0.7939 - acc: 0.73 - ETA: 30s - loss: 0.7332 - acc: 0.76 - ETA: 29s - loss: 0.6848 - acc: 0.80 - ETA: 28s - loss: 0.6642 - acc: 0.82 - ETA: 27s - loss: 0.6604 - acc: 0.82 - ETA: 27s - loss: 0.6492 - acc: 0.83 - ETA: 26s - loss: 0.6309 - acc: 0.84 - ETA: 25s - loss: 0.6234 - acc: 0.84 - ETA: 25s - loss: 0.6203 - acc: 0.84 - ETA: 24s - loss: 0.6240 - acc: 0.84 - ETA: 24s - loss: 0.6302 - acc: 0.83 - ETA: 23s - loss: 0.6299 - acc: 0.83 - ETA: 22s - loss: 0.6243 - acc: 0.84 - ETA: 22s - loss: 0.6261 - acc: 0.83 - ETA: 21s - loss: 0.6255 - acc: 0.83 - ETA: 20s - loss: 0.6246 - acc: 0.83 - ETA: 20s - loss: 0.6172 - acc: 0.84 - ETA: 19s - loss: 0.6148 - acc: 0.84 - ETA: 19s - loss: 0.6140 - acc: 0.84 - ETA: 18s - loss: 0.6129 - acc: 0.84 - ETA: 17s - loss: 0.6074 - acc: 0.84 - ETA: 17s - loss: 0.6002 - acc: 0.85 - ETA: 16s - loss: 0.5980 - acc: 0.85 - ETA: 15s - loss: 0.5971 - acc: 0.85 - ETA: 15s - loss: 0.5958 - acc: 0.85 - ETA: 14s - loss: 0.5907 - acc: 0.85 - ETA: 13s - loss: 0.5864 - acc: 0.86 - ETA: 13s - loss: 0.5850 - acc: 0.86 - ETA: 12s - loss: 0.5839 - acc: 0.86 - ETA: 11s - loss: 0.5835 - acc: 0.86 - ETA: 11s - loss: 0.5812 - acc: 0.86 - ETA: 10s - loss: 0.5828 - acc: 0.86 - ETA: 9s - loss: 0.5828 - acc: 0.8615 - ETA: 9s - loss: 0.5798 - acc: 0.863 - ETA: 8s - loss: 0.5784 - acc: 0.864 - ETA: 7s - loss: 0.5798 - acc: 0.861 - ETA: 7s - loss: 0.5790 - acc: 0.861 - ETA: 6s - loss: 0.5787 - acc: 0.861 - ETA: 5s - loss: 0.5794 - acc: 0.861 - ETA: 5s - loss: 0.5789 - acc: 0.861 - ETA: 4s - loss: 0.5764 - acc: 0.862 - ETA: 3s - loss: 0.5768 - acc: 0.863 - ETA: 3s - loss: 0.5752 - acc: 0.864 - ETA: 2s - loss: 0.5739 - acc: 0.865 - ETA: 1s - loss: 0.5744 - acc: 0.864 - ETA: 1s - loss: 0.5743 - acc: 0.864 - ETA: 0s - loss: 0.5753 - acc: 0.864 - 34s - loss: 0.5799 - acc: 0.8617 - val_loss: 0.7007 - val_acc: 0.7628\n",
      "Epoch 23/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.6529 - acc: 0.78 - ETA: 29s - loss: 0.6137 - acc: 0.81 - ETA: 28s - loss: 0.5869 - acc: 0.83 - ETA: 28s - loss: 0.5557 - acc: 0.85 - ETA: 27s - loss: 0.5469 - acc: 0.86 - ETA: 27s - loss: 0.5312 - acc: 0.87 - ETA: 26s - loss: 0.5283 - acc: 0.87 - ETA: 25s - loss: 0.5391 - acc: 0.86 - ETA: 25s - loss: 0.5336 - acc: 0.86 - ETA: 24s - loss: 0.5311 - acc: 0.87 - ETA: 24s - loss: 0.5311 - acc: 0.87 - ETA: 23s - loss: 0.5347 - acc: 0.86 - ETA: 22s - loss: 0.5333 - acc: 0.86 - ETA: 21s - loss: 0.5369 - acc: 0.86 - ETA: 21s - loss: 0.5335 - acc: 0.86 - ETA: 20s - loss: 0.5361 - acc: 0.86 - ETA: 19s - loss: 0.5347 - acc: 0.86 - ETA: 19s - loss: 0.5365 - acc: 0.86 - ETA: 18s - loss: 0.5351 - acc: 0.86 - ETA: 18s - loss: 0.5363 - acc: 0.86 - ETA: 17s - loss: 0.5340 - acc: 0.86 - ETA: 16s - loss: 0.5326 - acc: 0.86 - ETA: 16s - loss: 0.5367 - acc: 0.86 - ETA: 15s - loss: 0.5364 - acc: 0.86 - ETA: 14s - loss: 0.5356 - acc: 0.86 - ETA: 14s - loss: 0.5346 - acc: 0.86 - ETA: 13s - loss: 0.5350 - acc: 0.86 - ETA: 12s - loss: 0.5363 - acc: 0.86 - ETA: 12s - loss: 0.5351 - acc: 0.86 - ETA: 11s - loss: 0.5334 - acc: 0.86 - ETA: 10s - loss: 0.5328 - acc: 0.86 - ETA: 10s - loss: 0.5347 - acc: 0.86 - ETA: 9s - loss: 0.5350 - acc: 0.8606 - ETA: 8s - loss: 0.5342 - acc: 0.861 - ETA: 8s - loss: 0.5343 - acc: 0.861 - ETA: 7s - loss: 0.5339 - acc: 0.861 - ETA: 7s - loss: 0.5332 - acc: 0.861 - ETA: 6s - loss: 0.5319 - acc: 0.862 - ETA: 5s - loss: 0.5371 - acc: 0.858 - ETA: 5s - loss: 0.5444 - acc: 0.853 - ETA: 4s - loss: 0.5461 - acc: 0.852 - ETA: 3s - loss: 0.5454 - acc: 0.852 - ETA: 3s - loss: 0.5436 - acc: 0.853 - ETA: 2s - loss: 0.5437 - acc: 0.853 - ETA: 1s - loss: 0.5437 - acc: 0.853 - ETA: 1s - loss: 0.5425 - acc: 0.853 - ETA: 0s - loss: 0.5419 - acc: 0.853 - 33s - loss: 0.5415 - acc: 0.8529 - val_loss: 0.5534 - val_acc: 0.8441\n",
      "Epoch 24/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.4877 - acc: 0.88 - ETA: 29s - loss: 0.5176 - acc: 0.86 - ETA: 29s - loss: 0.5328 - acc: 0.85 - ETA: 30s - loss: 0.5183 - acc: 0.86 - ETA: 30s - loss: 0.5061 - acc: 0.86 - ETA: 29s - loss: 0.4904 - acc: 0.87 - ETA: 28s - loss: 0.4914 - acc: 0.88 - ETA: 27s - loss: 0.4998 - acc: 0.87 - ETA: 26s - loss: 0.5042 - acc: 0.86 - ETA: 26s - loss: 0.5055 - acc: 0.86 - ETA: 25s - loss: 0.5018 - acc: 0.86 - ETA: 24s - loss: 0.5012 - acc: 0.87 - ETA: 23s - loss: 0.4994 - acc: 0.87 - ETA: 23s - loss: 0.4949 - acc: 0.87 - ETA: 22s - loss: 0.4914 - acc: 0.87 - ETA: 21s - loss: 0.4937 - acc: 0.87 - ETA: 20s - loss: 0.4942 - acc: 0.87 - ETA: 20s - loss: 0.4983 - acc: 0.86 - ETA: 19s - loss: 0.4982 - acc: 0.86 - ETA: 18s - loss: 0.4989 - acc: 0.86 - ETA: 17s - loss: 0.4957 - acc: 0.86 - ETA: 17s - loss: 0.4948 - acc: 0.86 - ETA: 16s - loss: 0.4954 - acc: 0.86 - ETA: 15s - loss: 0.4946 - acc: 0.86 - ETA: 15s - loss: 0.4977 - acc: 0.86 - ETA: 14s - loss: 0.5032 - acc: 0.86 - ETA: 13s - loss: 0.5201 - acc: 0.85 - ETA: 13s - loss: 0.5263 - acc: 0.84 - ETA: 12s - loss: 0.5263 - acc: 0.84 - ETA: 11s - loss: 0.5258 - acc: 0.84 - ETA: 11s - loss: 0.5235 - acc: 0.84 - ETA: 10s - loss: 0.5222 - acc: 0.85 - ETA: 9s - loss: 0.5186 - acc: 0.8527 - ETA: 9s - loss: 0.5159 - acc: 0.854 - ETA: 8s - loss: 0.5158 - acc: 0.854 - ETA: 7s - loss: 0.5129 - acc: 0.856 - ETA: 7s - loss: 0.5120 - acc: 0.856 - ETA: 6s - loss: 0.5075 - acc: 0.858 - ETA: 5s - loss: 0.5059 - acc: 0.859 - ETA: 5s - loss: 0.5033 - acc: 0.860 - ETA: 4s - loss: 0.5024 - acc: 0.860 - ETA: 3s - loss: 0.5051 - acc: 0.858 - ETA: 3s - loss: 0.5082 - acc: 0.855 - ETA: 2s - loss: 0.5100 - acc: 0.853 - ETA: 1s - loss: 0.5100 - acc: 0.853 - ETA: 1s - loss: 0.5081 - acc: 0.854 - ETA: 0s - loss: 0.5074 - acc: 0.853 - 34s - loss: 0.5058 - acc: 0.8546 - val_loss: 0.5221 - val_acc: 0.8349\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 32s - loss: 0.4137 - acc: 0.89 - ETA: 31s - loss: 0.4446 - acc: 0.87 - ETA: 30s - loss: 0.4569 - acc: 0.88 - ETA: 30s - loss: 0.4544 - acc: 0.88 - ETA: 29s - loss: 0.4490 - acc: 0.88 - ETA: 28s - loss: 0.4510 - acc: 0.88 - ETA: 27s - loss: 0.4521 - acc: 0.88 - ETA: 27s - loss: 0.4421 - acc: 0.89 - ETA: 26s - loss: 0.4469 - acc: 0.89 - ETA: 25s - loss: 0.4433 - acc: 0.89 - ETA: 25s - loss: 0.4447 - acc: 0.89 - ETA: 24s - loss: 0.4415 - acc: 0.89 - ETA: 23s - loss: 0.4434 - acc: 0.88 - ETA: 23s - loss: 0.4417 - acc: 0.89 - ETA: 22s - loss: 0.4445 - acc: 0.88 - ETA: 22s - loss: 0.4410 - acc: 0.89 - ETA: 21s - loss: 0.4366 - acc: 0.89 - ETA: 20s - loss: 0.4413 - acc: 0.88 - ETA: 19s - loss: 0.4381 - acc: 0.88 - ETA: 19s - loss: 0.4356 - acc: 0.89 - ETA: 18s - loss: 0.4331 - acc: 0.89 - ETA: 17s - loss: 0.4350 - acc: 0.88 - ETA: 17s - loss: 0.4356 - acc: 0.88 - ETA: 16s - loss: 0.4393 - acc: 0.88 - ETA: 15s - loss: 0.4405 - acc: 0.88 - ETA: 15s - loss: 0.4407 - acc: 0.88 - ETA: 14s - loss: 0.4420 - acc: 0.88 - ETA: 13s - loss: 0.4440 - acc: 0.88 - ETA: 12s - loss: 0.4448 - acc: 0.88 - ETA: 12s - loss: 0.4464 - acc: 0.87 - ETA: 11s - loss: 0.4499 - acc: 0.87 - ETA: 10s - loss: 0.4521 - acc: 0.87 - ETA: 10s - loss: 0.4520 - acc: 0.87 - ETA: 9s - loss: 0.4515 - acc: 0.8759 - ETA: 8s - loss: 0.4496 - acc: 0.877 - ETA: 8s - loss: 0.4476 - acc: 0.878 - ETA: 7s - loss: 0.4474 - acc: 0.877 - ETA: 6s - loss: 0.4470 - acc: 0.877 - ETA: 5s - loss: 0.4478 - acc: 0.877 - ETA: 5s - loss: 0.4470 - acc: 0.877 - ETA: 4s - loss: 0.4474 - acc: 0.877 - ETA: 3s - loss: 0.4476 - acc: 0.878 - ETA: 3s - loss: 0.4456 - acc: 0.879 - ETA: 2s - loss: 0.4447 - acc: 0.879 - ETA: 1s - loss: 0.4463 - acc: 0.878 - ETA: 1s - loss: 0.4471 - acc: 0.877 - ETA: 0s - loss: 0.4464 - acc: 0.877 - 34s - loss: 0.4464 - acc: 0.8774 - val_loss: 0.4830 - val_acc: 0.8474\n",
      "Epoch 26/40\n",
      "4772/4772 [==============================] - ETA: 30s - loss: 0.3962 - acc: 0.90 - ETA: 29s - loss: 0.3832 - acc: 0.90 - ETA: 29s - loss: 0.3870 - acc: 0.89 - ETA: 28s - loss: 0.3854 - acc: 0.90 - ETA: 27s - loss: 0.3819 - acc: 0.89 - ETA: 27s - loss: 0.3769 - acc: 0.90 - ETA: 26s - loss: 0.3779 - acc: 0.89 - ETA: 25s - loss: 0.3954 - acc: 0.88 - ETA: 25s - loss: 0.4062 - acc: 0.88 - ETA: 24s - loss: 0.4026 - acc: 0.88 - ETA: 24s - loss: 0.4018 - acc: 0.89 - ETA: 23s - loss: 0.4026 - acc: 0.88 - ETA: 22s - loss: 0.3978 - acc: 0.89 - ETA: 22s - loss: 0.3973 - acc: 0.89 - ETA: 21s - loss: 0.4076 - acc: 0.88 - ETA: 21s - loss: 0.4194 - acc: 0.87 - ETA: 20s - loss: 0.4192 - acc: 0.87 - ETA: 19s - loss: 0.4171 - acc: 0.88 - ETA: 19s - loss: 0.4179 - acc: 0.87 - ETA: 18s - loss: 0.4243 - acc: 0.87 - ETA: 17s - loss: 0.4293 - acc: 0.87 - ETA: 17s - loss: 0.4300 - acc: 0.87 - ETA: 16s - loss: 0.4295 - acc: 0.87 - ETA: 15s - loss: 0.4296 - acc: 0.87 - ETA: 15s - loss: 0.4281 - acc: 0.87 - ETA: 14s - loss: 0.4269 - acc: 0.87 - ETA: 13s - loss: 0.4248 - acc: 0.87 - ETA: 13s - loss: 0.4232 - acc: 0.87 - ETA: 12s - loss: 0.4206 - acc: 0.87 - ETA: 11s - loss: 0.4212 - acc: 0.87 - ETA: 11s - loss: 0.4286 - acc: 0.87 - ETA: 10s - loss: 0.4292 - acc: 0.87 - ETA: 9s - loss: 0.4284 - acc: 0.8761 - ETA: 9s - loss: 0.4298 - acc: 0.875 - ETA: 8s - loss: 0.4285 - acc: 0.876 - ETA: 7s - loss: 0.4287 - acc: 0.875 - ETA: 7s - loss: 0.4307 - acc: 0.874 - ETA: 6s - loss: 0.4283 - acc: 0.876 - ETA: 5s - loss: 0.4269 - acc: 0.877 - ETA: 5s - loss: 0.4274 - acc: 0.877 - ETA: 4s - loss: 0.4271 - acc: 0.878 - ETA: 3s - loss: 0.4292 - acc: 0.876 - ETA: 3s - loss: 0.4303 - acc: 0.875 - ETA: 2s - loss: 0.4300 - acc: 0.875 - ETA: 1s - loss: 0.4316 - acc: 0.872 - ETA: 1s - loss: 0.4330 - acc: 0.871 - ETA: 0s - loss: 0.4342 - acc: 0.870 - 34s - loss: 0.4325 - acc: 0.8724 - val_loss: 0.4610 - val_acc: 0.8449\n",
      "Epoch 27/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3776 - acc: 0.88 - ETA: 29s - loss: 0.3776 - acc: 0.88 - ETA: 28s - loss: 0.3927 - acc: 0.88 - ETA: 27s - loss: 0.3883 - acc: 0.88 - ETA: 27s - loss: 0.3796 - acc: 0.89 - ETA: 26s - loss: 0.3966 - acc: 0.88 - ETA: 26s - loss: 0.4273 - acc: 0.86 - ETA: 25s - loss: 0.4558 - acc: 0.84 - ETA: 25s - loss: 0.4529 - acc: 0.84 - ETA: 24s - loss: 0.4448 - acc: 0.85 - ETA: 23s - loss: 0.4399 - acc: 0.85 - ETA: 23s - loss: 0.4343 - acc: 0.86 - ETA: 22s - loss: 0.4351 - acc: 0.86 - ETA: 21s - loss: 0.4328 - acc: 0.86 - ETA: 21s - loss: 0.4326 - acc: 0.86 - ETA: 20s - loss: 0.4318 - acc: 0.86 - ETA: 20s - loss: 0.4276 - acc: 0.87 - ETA: 19s - loss: 0.4255 - acc: 0.87 - ETA: 18s - loss: 0.4248 - acc: 0.87 - ETA: 18s - loss: 0.4232 - acc: 0.87 - ETA: 17s - loss: 0.4206 - acc: 0.87 - ETA: 16s - loss: 0.4154 - acc: 0.87 - ETA: 16s - loss: 0.4145 - acc: 0.87 - ETA: 15s - loss: 0.4137 - acc: 0.87 - ETA: 14s - loss: 0.4115 - acc: 0.87 - ETA: 14s - loss: 0.4119 - acc: 0.87 - ETA: 13s - loss: 0.4118 - acc: 0.87 - ETA: 12s - loss: 0.4136 - acc: 0.87 - ETA: 12s - loss: 0.4117 - acc: 0.87 - ETA: 11s - loss: 0.4123 - acc: 0.87 - ETA: 10s - loss: 0.4123 - acc: 0.87 - ETA: 10s - loss: 0.4102 - acc: 0.87 - ETA: 9s - loss: 0.4100 - acc: 0.8745 - ETA: 8s - loss: 0.4105 - acc: 0.875 - ETA: 8s - loss: 0.4088 - acc: 0.875 - ETA: 7s - loss: 0.4067 - acc: 0.876 - ETA: 7s - loss: 0.4056 - acc: 0.877 - ETA: 6s - loss: 0.4072 - acc: 0.875 - ETA: 5s - loss: 0.4130 - acc: 0.872 - ETA: 5s - loss: 0.4174 - acc: 0.871 - ETA: 4s - loss: 0.4192 - acc: 0.869 - ETA: 3s - loss: 0.4203 - acc: 0.868 - ETA: 3s - loss: 0.4197 - acc: 0.868 - ETA: 2s - loss: 0.4193 - acc: 0.868 - ETA: 1s - loss: 0.4187 - acc: 0.869 - ETA: 1s - loss: 0.4171 - acc: 0.870 - ETA: 0s - loss: 0.4148 - acc: 0.871 - 33s - loss: 0.4141 - acc: 0.8720 - val_loss: 0.4516 - val_acc: 0.8516\n",
      "Epoch 28/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3052 - acc: 0.94 - ETA: 30s - loss: 0.3586 - acc: 0.89 - ETA: 31s - loss: 0.4609 - acc: 0.84 - ETA: 30s - loss: 0.5122 - acc: 0.81 - ETA: 29s - loss: 0.5346 - acc: 0.79 - ETA: 28s - loss: 0.5312 - acc: 0.79 - ETA: 28s - loss: 0.5102 - acc: 0.80 - ETA: 27s - loss: 0.4935 - acc: 0.81 - ETA: 26s - loss: 0.4763 - acc: 0.82 - ETA: 25s - loss: 0.4702 - acc: 0.82 - ETA: 25s - loss: 0.4657 - acc: 0.83 - ETA: 24s - loss: 0.4566 - acc: 0.84 - ETA: 23s - loss: 0.4519 - acc: 0.84 - ETA: 22s - loss: 0.4477 - acc: 0.84 - ETA: 22s - loss: 0.4378 - acc: 0.85 - ETA: 21s - loss: 0.4379 - acc: 0.85 - ETA: 20s - loss: 0.4343 - acc: 0.85 - ETA: 19s - loss: 0.4343 - acc: 0.85 - ETA: 19s - loss: 0.4345 - acc: 0.85 - ETA: 18s - loss: 0.4331 - acc: 0.85 - ETA: 17s - loss: 0.4287 - acc: 0.85 - ETA: 17s - loss: 0.4269 - acc: 0.86 - ETA: 16s - loss: 0.4264 - acc: 0.86 - ETA: 15s - loss: 0.4242 - acc: 0.86 - ETA: 15s - loss: 0.4218 - acc: 0.86 - ETA: 14s - loss: 0.4178 - acc: 0.86 - ETA: 13s - loss: 0.4159 - acc: 0.86 - ETA: 13s - loss: 0.4145 - acc: 0.86 - ETA: 12s - loss: 0.4156 - acc: 0.86 - ETA: 11s - loss: 0.4187 - acc: 0.86 - ETA: 11s - loss: 0.4183 - acc: 0.86 - ETA: 10s - loss: 0.4197 - acc: 0.86 - ETA: 9s - loss: 0.4171 - acc: 0.8645 - ETA: 9s - loss: 0.4171 - acc: 0.865 - ETA: 8s - loss: 0.4156 - acc: 0.866 - ETA: 7s - loss: 0.4136 - acc: 0.867 - ETA: 7s - loss: 0.4129 - acc: 0.868 - ETA: 6s - loss: 0.4108 - acc: 0.869 - ETA: 5s - loss: 0.4111 - acc: 0.869 - ETA: 5s - loss: 0.4110 - acc: 0.869 - ETA: 4s - loss: 0.4098 - acc: 0.870 - ETA: 3s - loss: 0.4071 - acc: 0.872 - ETA: 3s - loss: 0.4051 - acc: 0.874 - ETA: 2s - loss: 0.4033 - acc: 0.875 - ETA: 1s - loss: 0.4027 - acc: 0.875 - ETA: 1s - loss: 0.4030 - acc: 0.875 - ETA: 0s - loss: 0.4020 - acc: 0.875 - 34s - loss: 0.4015 - acc: 0.8766 - val_loss: 0.4557 - val_acc: 0.8382\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 34s - loss: 0.3884 - acc: 0.87 - ETA: 31s - loss: 0.3400 - acc: 0.89 - ETA: 30s - loss: 0.3534 - acc: 0.89 - ETA: 29s - loss: 0.3559 - acc: 0.89 - ETA: 28s - loss: 0.3448 - acc: 0.90 - ETA: 28s - loss: 0.3421 - acc: 0.90 - ETA: 27s - loss: 0.3446 - acc: 0.90 - ETA: 26s - loss: 0.3532 - acc: 0.90 - ETA: 25s - loss: 0.3628 - acc: 0.89 - ETA: 25s - loss: 0.3621 - acc: 0.89 - ETA: 24s - loss: 0.3542 - acc: 0.90 - ETA: 23s - loss: 0.3488 - acc: 0.90 - ETA: 22s - loss: 0.3473 - acc: 0.90 - ETA: 22s - loss: 0.3457 - acc: 0.90 - ETA: 21s - loss: 0.3468 - acc: 0.90 - ETA: 20s - loss: 0.3487 - acc: 0.90 - ETA: 19s - loss: 0.3481 - acc: 0.90 - ETA: 19s - loss: 0.3485 - acc: 0.90 - ETA: 18s - loss: 0.3523 - acc: 0.89 - ETA: 17s - loss: 0.3541 - acc: 0.89 - ETA: 17s - loss: 0.3572 - acc: 0.89 - ETA: 16s - loss: 0.3596 - acc: 0.89 - ETA: 15s - loss: 0.3610 - acc: 0.89 - ETA: 15s - loss: 0.3619 - acc: 0.89 - ETA: 14s - loss: 0.3637 - acc: 0.89 - ETA: 14s - loss: 0.3631 - acc: 0.89 - ETA: 13s - loss: 0.3620 - acc: 0.89 - ETA: 12s - loss: 0.3637 - acc: 0.89 - ETA: 12s - loss: 0.3657 - acc: 0.88 - ETA: 11s - loss: 0.3656 - acc: 0.88 - ETA: 10s - loss: 0.3660 - acc: 0.88 - ETA: 10s - loss: 0.3683 - acc: 0.88 - ETA: 9s - loss: 0.3677 - acc: 0.8882 - ETA: 8s - loss: 0.3683 - acc: 0.888 - ETA: 8s - loss: 0.3664 - acc: 0.890 - ETA: 7s - loss: 0.3660 - acc: 0.890 - ETA: 6s - loss: 0.3686 - acc: 0.889 - ETA: 6s - loss: 0.3700 - acc: 0.887 - ETA: 5s - loss: 0.3696 - acc: 0.887 - ETA: 5s - loss: 0.3689 - acc: 0.887 - ETA: 4s - loss: 0.3687 - acc: 0.888 - ETA: 3s - loss: 0.3674 - acc: 0.889 - ETA: 3s - loss: 0.3691 - acc: 0.888 - ETA: 2s - loss: 0.3688 - acc: 0.888 - ETA: 1s - loss: 0.3708 - acc: 0.888 - ETA: 1s - loss: 0.3752 - acc: 0.886 - ETA: 0s - loss: 0.3759 - acc: 0.886 - 33s - loss: 0.3760 - acc: 0.8860 - val_loss: 0.4237 - val_acc: 0.8609\n",
      "Epoch 30/40\n",
      "4772/4772 [==============================] - ETA: 30s - loss: 0.3610 - acc: 0.88 - ETA: 29s - loss: 0.3750 - acc: 0.87 - ETA: 28s - loss: 0.3640 - acc: 0.88 - ETA: 27s - loss: 0.3490 - acc: 0.89 - ETA: 27s - loss: 0.3460 - acc: 0.89 - ETA: 26s - loss: 0.3372 - acc: 0.90 - ETA: 26s - loss: 0.3343 - acc: 0.90 - ETA: 25s - loss: 0.3340 - acc: 0.90 - ETA: 25s - loss: 0.3407 - acc: 0.90 - ETA: 24s - loss: 0.3394 - acc: 0.90 - ETA: 23s - loss: 0.3436 - acc: 0.90 - ETA: 23s - loss: 0.3463 - acc: 0.90 - ETA: 22s - loss: 0.3566 - acc: 0.89 - ETA: 21s - loss: 0.3711 - acc: 0.88 - ETA: 21s - loss: 0.3810 - acc: 0.87 - ETA: 20s - loss: 0.3840 - acc: 0.87 - ETA: 19s - loss: 0.3842 - acc: 0.87 - ETA: 19s - loss: 0.3814 - acc: 0.88 - ETA: 18s - loss: 0.3808 - acc: 0.88 - ETA: 17s - loss: 0.3769 - acc: 0.88 - ETA: 17s - loss: 0.3754 - acc: 0.88 - ETA: 16s - loss: 0.3741 - acc: 0.88 - ETA: 15s - loss: 0.3723 - acc: 0.88 - ETA: 15s - loss: 0.3725 - acc: 0.88 - ETA: 14s - loss: 0.3703 - acc: 0.88 - ETA: 13s - loss: 0.3686 - acc: 0.88 - ETA: 13s - loss: 0.3678 - acc: 0.88 - ETA: 12s - loss: 0.3666 - acc: 0.88 - ETA: 12s - loss: 0.3682 - acc: 0.88 - ETA: 11s - loss: 0.3692 - acc: 0.88 - ETA: 10s - loss: 0.3657 - acc: 0.88 - ETA: 10s - loss: 0.3658 - acc: 0.88 - ETA: 9s - loss: 0.3657 - acc: 0.8876 - ETA: 8s - loss: 0.3691 - acc: 0.886 - ETA: 8s - loss: 0.3690 - acc: 0.885 - ETA: 7s - loss: 0.3675 - acc: 0.886 - ETA: 6s - loss: 0.3669 - acc: 0.886 - ETA: 6s - loss: 0.3685 - acc: 0.887 - ETA: 5s - loss: 0.3679 - acc: 0.887 - ETA: 4s - loss: 0.3669 - acc: 0.887 - ETA: 4s - loss: 0.3654 - acc: 0.888 - ETA: 3s - loss: 0.3641 - acc: 0.889 - ETA: 3s - loss: 0.3637 - acc: 0.889 - ETA: 2s - loss: 0.3645 - acc: 0.889 - ETA: 1s - loss: 0.3655 - acc: 0.888 - ETA: 1s - loss: 0.3658 - acc: 0.888 - ETA: 0s - loss: 0.3664 - acc: 0.888 - 33s - loss: 0.3660 - acc: 0.8877 - val_loss: 0.4107 - val_acc: 0.8592\n",
      "Epoch 31/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3870 - acc: 0.86 - ETA: 29s - loss: 0.3558 - acc: 0.89 - ETA: 28s - loss: 0.3264 - acc: 0.91 - ETA: 27s - loss: 0.3062 - acc: 0.92 - ETA: 27s - loss: 0.3265 - acc: 0.91 - ETA: 26s - loss: 0.3291 - acc: 0.91 - ETA: 25s - loss: 0.3273 - acc: 0.91 - ETA: 25s - loss: 0.3356 - acc: 0.90 - ETA: 24s - loss: 0.3548 - acc: 0.89 - ETA: 24s - loss: 0.3687 - acc: 0.88 - ETA: 23s - loss: 0.3724 - acc: 0.87 - ETA: 22s - loss: 0.3735 - acc: 0.87 - ETA: 22s - loss: 0.3723 - acc: 0.87 - ETA: 21s - loss: 0.3685 - acc: 0.88 - ETA: 20s - loss: 0.3673 - acc: 0.88 - ETA: 20s - loss: 0.3670 - acc: 0.88 - ETA: 19s - loss: 0.3676 - acc: 0.88 - ETA: 18s - loss: 0.3702 - acc: 0.87 - ETA: 18s - loss: 0.3726 - acc: 0.87 - ETA: 17s - loss: 0.3752 - acc: 0.87 - ETA: 17s - loss: 0.3740 - acc: 0.87 - ETA: 16s - loss: 0.3690 - acc: 0.87 - ETA: 15s - loss: 0.3668 - acc: 0.88 - ETA: 15s - loss: 0.3661 - acc: 0.88 - ETA: 14s - loss: 0.3639 - acc: 0.88 - ETA: 13s - loss: 0.3611 - acc: 0.88 - ETA: 13s - loss: 0.3588 - acc: 0.88 - ETA: 12s - loss: 0.3557 - acc: 0.88 - ETA: 11s - loss: 0.3516 - acc: 0.88 - ETA: 11s - loss: 0.3487 - acc: 0.88 - ETA: 10s - loss: 0.3481 - acc: 0.89 - ETA: 10s - loss: 0.3472 - acc: 0.89 - ETA: 9s - loss: 0.3482 - acc: 0.8903 - ETA: 8s - loss: 0.3513 - acc: 0.886 - ETA: 8s - loss: 0.3552 - acc: 0.884 - ETA: 7s - loss: 0.3619 - acc: 0.881 - ETA: 6s - loss: 0.3653 - acc: 0.879 - ETA: 6s - loss: 0.3669 - acc: 0.878 - ETA: 5s - loss: 0.3688 - acc: 0.877 - ETA: 4s - loss: 0.3687 - acc: 0.878 - ETA: 4s - loss: 0.3692 - acc: 0.877 - ETA: 3s - loss: 0.3691 - acc: 0.877 - ETA: 3s - loss: 0.3689 - acc: 0.877 - ETA: 2s - loss: 0.3681 - acc: 0.877 - ETA: 1s - loss: 0.3677 - acc: 0.878 - ETA: 1s - loss: 0.3670 - acc: 0.878 - ETA: 0s - loss: 0.3703 - acc: 0.876 - 33s - loss: 0.3706 - acc: 0.8757 - val_loss: 0.4655 - val_acc: 0.8273\n",
      "Epoch 32/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3582 - acc: 0.87 - ETA: 28s - loss: 0.3480 - acc: 0.87 - ETA: 28s - loss: 0.3326 - acc: 0.89 - ETA: 28s - loss: 0.3423 - acc: 0.88 - ETA: 27s - loss: 0.3326 - acc: 0.89 - ETA: 26s - loss: 0.3265 - acc: 0.89 - ETA: 26s - loss: 0.3299 - acc: 0.89 - ETA: 25s - loss: 0.3346 - acc: 0.89 - ETA: 24s - loss: 0.3354 - acc: 0.89 - ETA: 24s - loss: 0.3376 - acc: 0.89 - ETA: 23s - loss: 0.3313 - acc: 0.89 - ETA: 22s - loss: 0.3309 - acc: 0.89 - ETA: 22s - loss: 0.3345 - acc: 0.89 - ETA: 21s - loss: 0.3357 - acc: 0.89 - ETA: 21s - loss: 0.3409 - acc: 0.89 - ETA: 20s - loss: 0.3378 - acc: 0.89 - ETA: 19s - loss: 0.3362 - acc: 0.89 - ETA: 19s - loss: 0.3354 - acc: 0.89 - ETA: 18s - loss: 0.3345 - acc: 0.89 - ETA: 17s - loss: 0.3324 - acc: 0.89 - ETA: 17s - loss: 0.3315 - acc: 0.89 - ETA: 16s - loss: 0.3329 - acc: 0.89 - ETA: 15s - loss: 0.3331 - acc: 0.89 - ETA: 15s - loss: 0.3346 - acc: 0.89 - ETA: 14s - loss: 0.3347 - acc: 0.89 - ETA: 13s - loss: 0.3351 - acc: 0.89 - ETA: 13s - loss: 0.3356 - acc: 0.89 - ETA: 12s - loss: 0.3346 - acc: 0.89 - ETA: 11s - loss: 0.3336 - acc: 0.89 - ETA: 11s - loss: 0.3332 - acc: 0.89 - ETA: 10s - loss: 0.3327 - acc: 0.89 - ETA: 10s - loss: 0.3413 - acc: 0.89 - ETA: 9s - loss: 0.3485 - acc: 0.8897 - ETA: 8s - loss: 0.3525 - acc: 0.887 - ETA: 8s - loss: 0.3531 - acc: 0.886 - ETA: 7s - loss: 0.3524 - acc: 0.886 - ETA: 6s - loss: 0.3519 - acc: 0.887 - ETA: 6s - loss: 0.3517 - acc: 0.888 - ETA: 5s - loss: 0.3517 - acc: 0.888 - ETA: 4s - loss: 0.3488 - acc: 0.890 - ETA: 4s - loss: 0.3474 - acc: 0.891 - ETA: 3s - loss: 0.3449 - acc: 0.892 - ETA: 3s - loss: 0.3461 - acc: 0.892 - ETA: 2s - loss: 0.3471 - acc: 0.891 - ETA: 1s - loss: 0.3473 - acc: 0.890 - ETA: 1s - loss: 0.3472 - acc: 0.891 - ETA: 0s - loss: 0.3466 - acc: 0.891 - 32s - loss: 0.3465 - acc: 0.8923 - val_loss: 0.4539 - val_acc: 0.8290\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3658 - acc: 0.86 - ETA: 28s - loss: 0.3876 - acc: 0.86 - ETA: 28s - loss: 0.3842 - acc: 0.86 - ETA: 27s - loss: 0.3702 - acc: 0.88 - ETA: 27s - loss: 0.3613 - acc: 0.88 - ETA: 26s - loss: 0.3493 - acc: 0.89 - ETA: 25s - loss: 0.3409 - acc: 0.89 - ETA: 25s - loss: 0.3380 - acc: 0.89 - ETA: 24s - loss: 0.3388 - acc: 0.89 - ETA: 23s - loss: 0.3344 - acc: 0.89 - ETA: 23s - loss: 0.3291 - acc: 0.89 - ETA: 22s - loss: 0.3317 - acc: 0.89 - ETA: 21s - loss: 0.3384 - acc: 0.89 - ETA: 21s - loss: 0.3491 - acc: 0.89 - ETA: 20s - loss: 0.3474 - acc: 0.89 - ETA: 20s - loss: 0.3453 - acc: 0.89 - ETA: 19s - loss: 0.3443 - acc: 0.89 - ETA: 18s - loss: 0.3457 - acc: 0.89 - ETA: 18s - loss: 0.3429 - acc: 0.89 - ETA: 17s - loss: 0.3399 - acc: 0.89 - ETA: 17s - loss: 0.3381 - acc: 0.90 - ETA: 16s - loss: 0.3369 - acc: 0.90 - ETA: 16s - loss: 0.3347 - acc: 0.90 - ETA: 15s - loss: 0.3369 - acc: 0.90 - ETA: 15s - loss: 0.3372 - acc: 0.90 - ETA: 14s - loss: 0.3405 - acc: 0.89 - ETA: 13s - loss: 0.3439 - acc: 0.89 - ETA: 13s - loss: 0.3430 - acc: 0.89 - ETA: 12s - loss: 0.3430 - acc: 0.89 - ETA: 11s - loss: 0.3439 - acc: 0.89 - ETA: 11s - loss: 0.3446 - acc: 0.89 - ETA: 10s - loss: 0.3455 - acc: 0.89 - ETA: 9s - loss: 0.3446 - acc: 0.8976 - ETA: 9s - loss: 0.3477 - acc: 0.895 - ETA: 8s - loss: 0.3543 - acc: 0.890 - ETA: 7s - loss: 0.3558 - acc: 0.889 - ETA: 7s - loss: 0.3564 - acc: 0.888 - ETA: 6s - loss: 0.3537 - acc: 0.891 - ETA: 5s - loss: 0.3523 - acc: 0.891 - ETA: 5s - loss: 0.3505 - acc: 0.892 - ETA: 4s - loss: 0.3493 - acc: 0.893 - ETA: 3s - loss: 0.3520 - acc: 0.893 - ETA: 3s - loss: 0.3523 - acc: 0.893 - ETA: 2s - loss: 0.3509 - acc: 0.893 - ETA: 1s - loss: 0.3495 - acc: 0.894 - ETA: 1s - loss: 0.3484 - acc: 0.894 - ETA: 0s - loss: 0.3480 - acc: 0.894 - 33s - loss: 0.3473 - acc: 0.8954 - val_loss: 0.3977 - val_acc: 0.8634\n",
      "Epoch 34/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.2875 - acc: 0.92 - ETA: 28s - loss: 0.2860 - acc: 0.92 - ETA: 28s - loss: 0.3037 - acc: 0.90 - ETA: 27s - loss: 0.3337 - acc: 0.89 - ETA: 26s - loss: 0.3397 - acc: 0.88 - ETA: 26s - loss: 0.3412 - acc: 0.88 - ETA: 25s - loss: 0.3284 - acc: 0.89 - ETA: 25s - loss: 0.3257 - acc: 0.89 - ETA: 24s - loss: 0.3190 - acc: 0.90 - ETA: 23s - loss: 0.3189 - acc: 0.90 - ETA: 23s - loss: 0.3188 - acc: 0.90 - ETA: 22s - loss: 0.3165 - acc: 0.90 - ETA: 21s - loss: 0.3119 - acc: 0.90 - ETA: 21s - loss: 0.3158 - acc: 0.90 - ETA: 20s - loss: 0.3173 - acc: 0.90 - ETA: 20s - loss: 0.3208 - acc: 0.90 - ETA: 19s - loss: 0.3241 - acc: 0.90 - ETA: 18s - loss: 0.3222 - acc: 0.90 - ETA: 18s - loss: 0.3206 - acc: 0.90 - ETA: 17s - loss: 0.3217 - acc: 0.90 - ETA: 16s - loss: 0.3175 - acc: 0.90 - ETA: 16s - loss: 0.3162 - acc: 0.90 - ETA: 15s - loss: 0.3138 - acc: 0.91 - ETA: 14s - loss: 0.3134 - acc: 0.91 - ETA: 14s - loss: 0.3144 - acc: 0.90 - ETA: 13s - loss: 0.3220 - acc: 0.90 - ETA: 13s - loss: 0.3296 - acc: 0.89 - ETA: 12s - loss: 0.3342 - acc: 0.89 - ETA: 11s - loss: 0.3342 - acc: 0.89 - ETA: 11s - loss: 0.3337 - acc: 0.89 - ETA: 10s - loss: 0.3341 - acc: 0.89 - ETA: 9s - loss: 0.3336 - acc: 0.8963 - ETA: 9s - loss: 0.3337 - acc: 0.896 - ETA: 8s - loss: 0.3325 - acc: 0.897 - ETA: 8s - loss: 0.3328 - acc: 0.897 - ETA: 7s - loss: 0.3315 - acc: 0.899 - ETA: 6s - loss: 0.3316 - acc: 0.899 - ETA: 6s - loss: 0.3307 - acc: 0.898 - ETA: 5s - loss: 0.3317 - acc: 0.898 - ETA: 4s - loss: 0.3345 - acc: 0.897 - ETA: 4s - loss: 0.3382 - acc: 0.895 - ETA: 3s - loss: 0.3410 - acc: 0.894 - ETA: 2s - loss: 0.3402 - acc: 0.894 - ETA: 2s - loss: 0.3387 - acc: 0.896 - ETA: 1s - loss: 0.3383 - acc: 0.896 - ETA: 1s - loss: 0.3357 - acc: 0.898 - ETA: 0s - loss: 0.3358 - acc: 0.898 - 32s - loss: 0.3362 - acc: 0.8984 - val_loss: 0.3819 - val_acc: 0.8650\n",
      "Epoch 35/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3429 - acc: 0.91 - ETA: 28s - loss: 0.3269 - acc: 0.90 - ETA: 27s - loss: 0.3521 - acc: 0.88 - ETA: 27s - loss: 0.3420 - acc: 0.89 - ETA: 26s - loss: 0.3308 - acc: 0.89 - ETA: 26s - loss: 0.3323 - acc: 0.89 - ETA: 25s - loss: 0.3253 - acc: 0.90 - ETA: 24s - loss: 0.3217 - acc: 0.90 - ETA: 24s - loss: 0.3167 - acc: 0.90 - ETA: 23s - loss: 0.3136 - acc: 0.90 - ETA: 23s - loss: 0.3139 - acc: 0.90 - ETA: 22s - loss: 0.3129 - acc: 0.90 - ETA: 21s - loss: 0.3163 - acc: 0.90 - ETA: 21s - loss: 0.3284 - acc: 0.90 - ETA: 20s - loss: 0.3456 - acc: 0.88 - ETA: 20s - loss: 0.3554 - acc: 0.88 - ETA: 19s - loss: 0.3557 - acc: 0.88 - ETA: 18s - loss: 0.3561 - acc: 0.88 - ETA: 18s - loss: 0.3539 - acc: 0.88 - ETA: 17s - loss: 0.3509 - acc: 0.88 - ETA: 16s - loss: 0.3491 - acc: 0.89 - ETA: 16s - loss: 0.3471 - acc: 0.89 - ETA: 15s - loss: 0.3439 - acc: 0.89 - ETA: 15s - loss: 0.3409 - acc: 0.89 - ETA: 14s - loss: 0.3387 - acc: 0.89 - ETA: 13s - loss: 0.3384 - acc: 0.89 - ETA: 13s - loss: 0.3420 - acc: 0.89 - ETA: 12s - loss: 0.3463 - acc: 0.89 - ETA: 11s - loss: 0.3485 - acc: 0.88 - ETA: 11s - loss: 0.3496 - acc: 0.88 - ETA: 10s - loss: 0.3508 - acc: 0.88 - ETA: 10s - loss: 0.3501 - acc: 0.89 - ETA: 9s - loss: 0.3480 - acc: 0.8912 - ETA: 8s - loss: 0.3478 - acc: 0.891 - ETA: 8s - loss: 0.3466 - acc: 0.892 - ETA: 7s - loss: 0.3464 - acc: 0.893 - ETA: 6s - loss: 0.3458 - acc: 0.893 - ETA: 6s - loss: 0.3445 - acc: 0.894 - ETA: 5s - loss: 0.3455 - acc: 0.893 - ETA: 4s - loss: 0.3446 - acc: 0.894 - ETA: 4s - loss: 0.3438 - acc: 0.894 - ETA: 3s - loss: 0.3424 - acc: 0.895 - ETA: 3s - loss: 0.3417 - acc: 0.895 - ETA: 2s - loss: 0.3400 - acc: 0.896 - ETA: 1s - loss: 0.3394 - acc: 0.896 - ETA: 1s - loss: 0.3414 - acc: 0.895 - ETA: 0s - loss: 0.3447 - acc: 0.894 - 33s - loss: 0.3444 - acc: 0.8940 - val_loss: 0.3877 - val_acc: 0.8684\n",
      "Epoch 36/40\n",
      "4772/4772 [==============================] - ETA: 32s - loss: 0.2629 - acc: 0.93 - ETA: 30s - loss: 0.2645 - acc: 0.93 - ETA: 29s - loss: 0.2554 - acc: 0.93 - ETA: 28s - loss: 0.2568 - acc: 0.93 - ETA: 28s - loss: 0.2646 - acc: 0.93 - ETA: 28s - loss: 0.2755 - acc: 0.92 - ETA: 27s - loss: 0.2782 - acc: 0.92 - ETA: 26s - loss: 0.2845 - acc: 0.92 - ETA: 26s - loss: 0.2955 - acc: 0.91 - ETA: 25s - loss: 0.3075 - acc: 0.91 - ETA: 25s - loss: 0.3181 - acc: 0.90 - ETA: 24s - loss: 0.3199 - acc: 0.90 - ETA: 23s - loss: 0.3206 - acc: 0.90 - ETA: 22s - loss: 0.3168 - acc: 0.90 - ETA: 22s - loss: 0.3144 - acc: 0.90 - ETA: 21s - loss: 0.3128 - acc: 0.90 - ETA: 20s - loss: 0.3109 - acc: 0.91 - ETA: 19s - loss: 0.3081 - acc: 0.91 - ETA: 19s - loss: 0.3039 - acc: 0.91 - ETA: 18s - loss: 0.3013 - acc: 0.91 - ETA: 18s - loss: 0.2997 - acc: 0.91 - ETA: 17s - loss: 0.3058 - acc: 0.91 - ETA: 16s - loss: 0.3151 - acc: 0.90 - ETA: 16s - loss: 0.3172 - acc: 0.90 - ETA: 15s - loss: 0.3181 - acc: 0.90 - ETA: 14s - loss: 0.3179 - acc: 0.90 - ETA: 14s - loss: 0.3158 - acc: 0.90 - ETA: 13s - loss: 0.3138 - acc: 0.90 - ETA: 13s - loss: 0.3158 - acc: 0.90 - ETA: 12s - loss: 0.3148 - acc: 0.90 - ETA: 11s - loss: 0.3144 - acc: 0.90 - ETA: 11s - loss: 0.3118 - acc: 0.90 - ETA: 10s - loss: 0.3122 - acc: 0.90 - ETA: 9s - loss: 0.3123 - acc: 0.9082 - ETA: 9s - loss: 0.3132 - acc: 0.907 - ETA: 8s - loss: 0.3131 - acc: 0.907 - ETA: 7s - loss: 0.3123 - acc: 0.907 - ETA: 6s - loss: 0.3124 - acc: 0.907 - ETA: 6s - loss: 0.3121 - acc: 0.906 - ETA: 5s - loss: 0.3120 - acc: 0.907 - ETA: 4s - loss: 0.3115 - acc: 0.907 - ETA: 4s - loss: 0.3111 - acc: 0.907 - ETA: 3s - loss: 0.3117 - acc: 0.907 - ETA: 2s - loss: 0.3133 - acc: 0.906 - ETA: 1s - loss: 0.3187 - acc: 0.903 - ETA: 1s - loss: 0.3302 - acc: 0.898 - ETA: 0s - loss: 0.3362 - acc: 0.893 - 37s - loss: 0.3374 - acc: 0.8929 - val_loss: 0.4031 - val_acc: 0.8718\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772/4772 [==============================] - ETA: 32s - loss: 0.2883 - acc: 0.95 - ETA: 30s - loss: 0.2789 - acc: 0.94 - ETA: 30s - loss: 0.2787 - acc: 0.94 - ETA: 30s - loss: 0.2885 - acc: 0.93 - ETA: 31s - loss: 0.2968 - acc: 0.93 - ETA: 31s - loss: 0.2897 - acc: 0.93 - ETA: 30s - loss: 0.3081 - acc: 0.92 - ETA: 30s - loss: 0.3118 - acc: 0.91 - ETA: 29s - loss: 0.3199 - acc: 0.91 - ETA: 28s - loss: 0.3209 - acc: 0.91 - ETA: 27s - loss: 0.3169 - acc: 0.91 - ETA: 26s - loss: 0.3177 - acc: 0.91 - ETA: 26s - loss: 0.3146 - acc: 0.91 - ETA: 25s - loss: 0.3138 - acc: 0.91 - ETA: 24s - loss: 0.3125 - acc: 0.91 - ETA: 24s - loss: 0.3135 - acc: 0.91 - ETA: 23s - loss: 0.3131 - acc: 0.91 - ETA: 22s - loss: 0.3096 - acc: 0.91 - ETA: 21s - loss: 0.3095 - acc: 0.91 - ETA: 20s - loss: 0.3119 - acc: 0.91 - ETA: 19s - loss: 0.3180 - acc: 0.91 - ETA: 19s - loss: 0.3210 - acc: 0.91 - ETA: 18s - loss: 0.3225 - acc: 0.90 - ETA: 17s - loss: 0.3226 - acc: 0.90 - ETA: 16s - loss: 0.3246 - acc: 0.90 - ETA: 15s - loss: 0.3272 - acc: 0.90 - ETA: 15s - loss: 0.3275 - acc: 0.90 - ETA: 14s - loss: 0.3264 - acc: 0.90 - ETA: 13s - loss: 0.3256 - acc: 0.90 - ETA: 12s - loss: 0.3259 - acc: 0.90 - ETA: 12s - loss: 0.3242 - acc: 0.90 - ETA: 11s - loss: 0.3237 - acc: 0.90 - ETA: 10s - loss: 0.3219 - acc: 0.90 - ETA: 9s - loss: 0.3212 - acc: 0.9079 - ETA: 9s - loss: 0.3195 - acc: 0.908 - ETA: 8s - loss: 0.3178 - acc: 0.909 - ETA: 7s - loss: 0.3157 - acc: 0.910 - ETA: 6s - loss: 0.3158 - acc: 0.910 - ETA: 6s - loss: 0.3187 - acc: 0.909 - ETA: 5s - loss: 0.3231 - acc: 0.907 - ETA: 4s - loss: 0.3247 - acc: 0.905 - ETA: 4s - loss: 0.3255 - acc: 0.904 - ETA: 3s - loss: 0.3266 - acc: 0.904 - ETA: 2s - loss: 0.3263 - acc: 0.903 - ETA: 1s - loss: 0.3263 - acc: 0.903 - ETA: 1s - loss: 0.3278 - acc: 0.902 - ETA: 0s - loss: 0.3305 - acc: 0.900 - 36s - loss: 0.3310 - acc: 0.8994 - val_loss: 0.4256 - val_acc: 0.8508\n",
      "Epoch 38/40\n",
      "4772/4772 [==============================] - ETA: 30s - loss: 0.3320 - acc: 0.90 - ETA: 30s - loss: 0.3412 - acc: 0.89 - ETA: 29s - loss: 0.3295 - acc: 0.89 - ETA: 29s - loss: 0.3292 - acc: 0.89 - ETA: 29s - loss: 0.3073 - acc: 0.91 - ETA: 28s - loss: 0.3012 - acc: 0.91 - ETA: 28s - loss: 0.2973 - acc: 0.91 - ETA: 27s - loss: 0.3077 - acc: 0.91 - ETA: 26s - loss: 0.3061 - acc: 0.91 - ETA: 25s - loss: 0.3070 - acc: 0.91 - ETA: 25s - loss: 0.3060 - acc: 0.91 - ETA: 24s - loss: 0.3047 - acc: 0.91 - ETA: 23s - loss: 0.3004 - acc: 0.92 - ETA: 22s - loss: 0.3021 - acc: 0.91 - ETA: 22s - loss: 0.3016 - acc: 0.92 - ETA: 21s - loss: 0.3023 - acc: 0.91 - ETA: 20s - loss: 0.3007 - acc: 0.91 - ETA: 19s - loss: 0.2967 - acc: 0.92 - ETA: 19s - loss: 0.2943 - acc: 0.92 - ETA: 18s - loss: 0.2904 - acc: 0.92 - ETA: 17s - loss: 0.2896 - acc: 0.92 - ETA: 17s - loss: 0.2901 - acc: 0.92 - ETA: 16s - loss: 0.2897 - acc: 0.92 - ETA: 15s - loss: 0.2907 - acc: 0.92 - ETA: 15s - loss: 0.2941 - acc: 0.91 - ETA: 14s - loss: 0.2975 - acc: 0.91 - ETA: 13s - loss: 0.2980 - acc: 0.91 - ETA: 13s - loss: 0.2976 - acc: 0.91 - ETA: 12s - loss: 0.2988 - acc: 0.91 - ETA: 11s - loss: 0.2984 - acc: 0.91 - ETA: 11s - loss: 0.3014 - acc: 0.91 - ETA: 10s - loss: 0.3015 - acc: 0.91 - ETA: 9s - loss: 0.3016 - acc: 0.9170 - ETA: 9s - loss: 0.3022 - acc: 0.917 - ETA: 8s - loss: 0.3026 - acc: 0.916 - ETA: 7s - loss: 0.3028 - acc: 0.916 - ETA: 7s - loss: 0.3038 - acc: 0.915 - ETA: 6s - loss: 0.3030 - acc: 0.915 - ETA: 5s - loss: 0.3023 - acc: 0.915 - ETA: 5s - loss: 0.3012 - acc: 0.916 - ETA: 4s - loss: 0.3022 - acc: 0.914 - ETA: 3s - loss: 0.3056 - acc: 0.911 - ETA: 3s - loss: 0.3090 - acc: 0.909 - ETA: 2s - loss: 0.3097 - acc: 0.909 - ETA: 1s - loss: 0.3097 - acc: 0.910 - ETA: 1s - loss: 0.3094 - acc: 0.910 - ETA: 0s - loss: 0.3079 - acc: 0.911 - 34s - loss: 0.3070 - acc: 0.9120 - val_loss: 0.3859 - val_acc: 0.8692\n",
      "Epoch 39/40\n",
      "4772/4772 [==============================] - ETA: 30s - loss: 0.2900 - acc: 0.94 - ETA: 30s - loss: 0.2581 - acc: 0.95 - ETA: 29s - loss: 0.2748 - acc: 0.93 - ETA: 28s - loss: 0.2739 - acc: 0.93 - ETA: 27s - loss: 0.2915 - acc: 0.92 - ETA: 27s - loss: 0.3050 - acc: 0.91 - ETA: 26s - loss: 0.3123 - acc: 0.91 - ETA: 25s - loss: 0.3124 - acc: 0.91 - ETA: 25s - loss: 0.3138 - acc: 0.91 - ETA: 24s - loss: 0.3083 - acc: 0.91 - ETA: 23s - loss: 0.3107 - acc: 0.91 - ETA: 23s - loss: 0.3155 - acc: 0.90 - ETA: 22s - loss: 0.3139 - acc: 0.91 - ETA: 22s - loss: 0.3107 - acc: 0.91 - ETA: 21s - loss: 0.3094 - acc: 0.91 - ETA: 20s - loss: 0.3201 - acc: 0.90 - ETA: 20s - loss: 0.3294 - acc: 0.90 - ETA: 19s - loss: 0.3315 - acc: 0.90 - ETA: 18s - loss: 0.3283 - acc: 0.90 - ETA: 18s - loss: 0.3253 - acc: 0.90 - ETA: 17s - loss: 0.3219 - acc: 0.90 - ETA: 16s - loss: 0.3223 - acc: 0.90 - ETA: 16s - loss: 0.3223 - acc: 0.90 - ETA: 15s - loss: 0.3193 - acc: 0.90 - ETA: 14s - loss: 0.3184 - acc: 0.90 - ETA: 14s - loss: 0.3154 - acc: 0.91 - ETA: 13s - loss: 0.3173 - acc: 0.91 - ETA: 12s - loss: 0.3213 - acc: 0.90 - ETA: 12s - loss: 0.3235 - acc: 0.90 - ETA: 11s - loss: 0.3214 - acc: 0.90 - ETA: 10s - loss: 0.3196 - acc: 0.90 - ETA: 10s - loss: 0.3222 - acc: 0.90 - ETA: 9s - loss: 0.3203 - acc: 0.9091 - ETA: 9s - loss: 0.3174 - acc: 0.910 - ETA: 8s - loss: 0.3152 - acc: 0.911 - ETA: 7s - loss: 0.3139 - acc: 0.912 - ETA: 7s - loss: 0.3122 - acc: 0.913 - ETA: 6s - loss: 0.3101 - acc: 0.914 - ETA: 5s - loss: 0.3107 - acc: 0.913 - ETA: 5s - loss: 0.3101 - acc: 0.914 - ETA: 4s - loss: 0.3095 - acc: 0.914 - ETA: 3s - loss: 0.3092 - acc: 0.914 - ETA: 3s - loss: 0.3077 - acc: 0.915 - ETA: 2s - loss: 0.3071 - acc: 0.915 - ETA: 1s - loss: 0.3078 - acc: 0.913 - ETA: 1s - loss: 0.3133 - acc: 0.911 - ETA: 0s - loss: 0.3160 - acc: 0.909 - 33s - loss: 0.3158 - acc: 0.9101 - val_loss: 0.3715 - val_acc: 0.8759\n",
      "Epoch 40/40\n",
      "4772/4772 [==============================] - ETA: 29s - loss: 0.3074 - acc: 0.89 - ETA: 29s - loss: 0.3110 - acc: 0.89 - ETA: 28s - loss: 0.3095 - acc: 0.90 - ETA: 28s - loss: 0.2923 - acc: 0.91 - ETA: 27s - loss: 0.2888 - acc: 0.91 - ETA: 27s - loss: 0.2715 - acc: 0.92 - ETA: 26s - loss: 0.2708 - acc: 0.92 - ETA: 25s - loss: 0.2726 - acc: 0.92 - ETA: 25s - loss: 0.2755 - acc: 0.91 - ETA: 24s - loss: 0.2787 - acc: 0.92 - ETA: 23s - loss: 0.2933 - acc: 0.91 - ETA: 23s - loss: 0.3057 - acc: 0.90 - ETA: 22s - loss: 0.3085 - acc: 0.90 - ETA: 21s - loss: 0.3103 - acc: 0.89 - ETA: 21s - loss: 0.3053 - acc: 0.90 - ETA: 20s - loss: 0.3032 - acc: 0.90 - ETA: 20s - loss: 0.3013 - acc: 0.90 - ETA: 19s - loss: 0.3007 - acc: 0.90 - ETA: 19s - loss: 0.2977 - acc: 0.90 - ETA: 18s - loss: 0.2973 - acc: 0.90 - ETA: 17s - loss: 0.2956 - acc: 0.91 - ETA: 17s - loss: 0.2947 - acc: 0.91 - ETA: 16s - loss: 0.2921 - acc: 0.91 - ETA: 15s - loss: 0.2964 - acc: 0.91 - ETA: 15s - loss: 0.2963 - acc: 0.91 - ETA: 14s - loss: 0.2953 - acc: 0.91 - ETA: 13s - loss: 0.2951 - acc: 0.91 - ETA: 13s - loss: 0.2928 - acc: 0.91 - ETA: 12s - loss: 0.2962 - acc: 0.91 - ETA: 11s - loss: 0.2972 - acc: 0.91 - ETA: 11s - loss: 0.3036 - acc: 0.91 - ETA: 10s - loss: 0.3078 - acc: 0.90 - ETA: 9s - loss: 0.3080 - acc: 0.9088 - ETA: 9s - loss: 0.3093 - acc: 0.907 - ETA: 8s - loss: 0.3080 - acc: 0.908 - ETA: 7s - loss: 0.3073 - acc: 0.909 - ETA: 7s - loss: 0.3076 - acc: 0.908 - ETA: 6s - loss: 0.3088 - acc: 0.907 - ETA: 5s - loss: 0.3078 - acc: 0.908 - ETA: 5s - loss: 0.3077 - acc: 0.908 - ETA: 4s - loss: 0.3069 - acc: 0.908 - ETA: 3s - loss: 0.3062 - acc: 0.908 - ETA: 3s - loss: 0.3056 - acc: 0.908 - ETA: 2s - loss: 0.3051 - acc: 0.909 - ETA: 1s - loss: 0.3044 - acc: 0.910 - ETA: 1s - loss: 0.3050 - acc: 0.910 - ETA: 0s - loss: 0.3057 - acc: 0.909 - 34s - loss: 0.3048 - acc: 0.9101 - val_loss: 0.3622 - val_acc: 0.8759\n"
     ]
    }
   ],
   "source": [
    "model = nn_model()\n",
    "history = model.fit(x_train_and_validation_new, y_train_and_validation, validation_data=(x_test_new, y_test), verbose=1, epochs=EPOCHS, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193/1193 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3621779902333086, 0.8759430099313037]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "score = model.evaluate(x_test_new, y_test, batch_size=BATCH_SIZE)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 24)        1176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 48)        18480     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              12583936  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 12,604,617\n",
      "Trainable params: 12,604,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQggh3KQoCCGEcPv/AVY9P0ZuBE12AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25faac18f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curves for validation and training data during learning\n",
    "show_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE MODEL TO A FILE\n",
    "model.save(\"cnn_88_03.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.944558521560575\n",
      "recall =  0.9504132231404959\n",
      "accuracy =  0.946596858639\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual 1</th>\n",
       "      <th>actual 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted 1</th>\n",
       "      <td>460</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted 0</th>\n",
       "      <td>24</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual 1  actual 0\n",
       "predicted 1       460        27\n",
       "predicted 0        24       444"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_new_validation)\n",
    "y_pred = np.round(y_pred.reshape(-1))\n",
    "\n",
    "TP = np.count_nonzero(y_pred * y_validation)\n",
    "TN = np.count_nonzero((y_pred - 1) * (y_validation - 1))\n",
    "FP = np.count_nonzero(y_pred * (y_validation - 1))\n",
    "FN = np.count_nonzero((y_pred - 1) * y_validation)\n",
    "\n",
    "confusion_matrix_dict = {'actual 1': [TP, FN], 'actual 0': [FP, TN]}\n",
    "confusion_matrix = pd.DataFrame(data=confusion_matrix_dict, columns =['actual 1', 'actual 0'], index=['predicted 1', 'predicted 0'])\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "accuracy = metrics.accuracy_score(y_validation, y_pred)\n",
    "\n",
    "print('precision = ', precision)\n",
    "print('recall = ', recall)\n",
    "print('accuracy = ', accuracy)\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.8780487804878049\n",
      "recall =  0.8809135399673735\n",
      "accuracy =  0.875943000838\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual 1</th>\n",
       "      <th>actual 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted 1</th>\n",
       "      <td>540</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted 0</th>\n",
       "      <td>73</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual 1  actual 0\n",
       "predicted 1       540        75\n",
       "predicted 0        73       505"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model.predict(x_test_new)\n",
    "y_pred_test = np.round(y_pred_test.reshape(-1,))\n",
    "\n",
    "TP = np.count_nonzero(y_pred_test * y_test)\n",
    "TN = np.count_nonzero((y_pred_test - 1) * (y_test - 1))\n",
    "FP = np.count_nonzero(y_pred_test * (y_test - 1))\n",
    "FN = np.count_nonzero((y_pred_test - 1) * y_test)\n",
    "\n",
    "confusion_matrix_dict = {'actual 1': [TP, FN], 'actual 0': [FP, TN]}\n",
    "confusion_matrix = pd.DataFrame(data=confusion_matrix_dict, columns =['actual 1', 'actual 0'], index=['predicted 1', 'predicted 0'])\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print('precision = ', precision)\n",
    "print('recall = ', recall)\n",
    "print('accuracy = ', accuracy)\n",
    "\n",
    "confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47262713]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = open_image('2.jpg')\n",
    "resized_img = resize(img)\n",
    "array = convert_to_array(resized_img)/255\n",
    "array = array.reshape((-1,64,64,3))\n",
    "model.predict(array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
